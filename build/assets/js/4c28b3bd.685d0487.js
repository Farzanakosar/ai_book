"use strict";(globalThis.webpackChunkros2_book=globalThis.webpackChunkros2_book||[]).push([[437],{8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var r=i(6540);const t={},o=r.createContext(t);function s(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),r.createElement(o.Provider,{value:n},e.children)}},8553(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"vla/whisper-integration","title":"OpenAI Whisper Integration Guide","description":"Overview","source":"@site/docs/vla/whisper-integration.md","sourceDirName":"vla","slug":"/vla/whisper-integration","permalink":"/docs/vla/whisper-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/whisper-integration.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action Prerequisites Guide","permalink":"/docs/vla/prerequisites"},"next":{"title":"Voice Command Mapping Guide","permalink":"/docs/vla/voice-command-mapping"}}');var t=i(4848),o=i(8453);const s={},a="OpenAI Whisper Integration Guide",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Basic Whisper Integration",id:"basic-whisper-integration",level:2},{value:"1. Setting Up the OpenAI Client",id:"1-setting-up-the-openai-client",level:3},{value:"2. Transcribing Audio Files",id:"2-transcribing-audio-files",level:3},{value:"3. Supported Audio Formats",id:"3-supported-audio-formats",level:3},{value:"Robotics-Specific Implementation",id:"robotics-specific-implementation",level:2},{value:"1. Real-time Audio Processing",id:"1-real-time-audio-processing",level:3},{value:"2. Batch Processing for Better Performance",id:"2-batch-processing-for-better-performance",level:3},{value:"Error Handling and Best Practices",id:"error-handling-and-best-practices",level:2},{value:"1. Rate Limiting",id:"1-rate-limiting",level:3},{value:"2. Audio Quality Considerations",id:"2-audio-quality-considerations",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Caching Common Commands",id:"1-caching-common-commands",level:3},{value:"2. Model Selection",id:"2-model-selection",level:3},{value:"Integration with Robot Actions",id:"integration-with-robot-actions",level:2},{value:"Testing Your Integration",id:"testing-your-integration",level:2},{value:"1. Unit Tests",id:"1-unit-tests",level:3},{value:"2. Integration Tests",id:"2-integration-tests",level:3},{value:"Troubleshooting Common Issues",id:"troubleshooting-common-issues",level:2},{value:"1. API Connection Errors",id:"1-api-connection-errors",level:3},{value:"2. Poor Transcription Quality",id:"2-poor-transcription-quality",level:3},{value:"3. High Latency",id:"3-high-latency",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"openai-whisper-integration-guide",children:"OpenAI Whisper Integration Guide"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"This guide covers how to integrate OpenAI's Whisper API for speech recognition in your robotics applications. Whisper is a state-of-the-art automatic speech recognition (ASR) system that can convert spoken language into written text with high accuracy."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before implementing Whisper integration, ensure you have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"An OpenAI API key"}),"\n",(0,t.jsx)(n.li,{children:"Python 3.8 or higher installed"}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"openai"})," Python package installed (",(0,t.jsx)(n.code,{children:"pip install openai"}),")"]}),"\n",(0,t.jsx)(n.li,{children:"Basic understanding of REST API integration"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"basic-whisper-integration",children:"Basic Whisper Integration"}),"\n",(0,t.jsx)(n.h3,{id:"1-setting-up-the-openai-client",children:"1. Setting Up the OpenAI Client"}),"\n",(0,t.jsx)(n.p,{children:"First, configure the OpenAI client with your API key:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nimport os\n\n# Set your OpenAI API key\nopenai.api_key = os.getenv("OPENAI_API_KEY")\n\n# Alternative: directly assign the API key\n# openai.api_key = "your-api-key-here"\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-transcribing-audio-files",children:"2. Transcribing Audio Files"}),"\n",(0,t.jsx)(n.p,{children:"Whisper can transcribe audio files in several formats. Here's a basic example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nfrom pathlib import Path\n\n# Transcribe an audio file\naudio_file = open("path/to/your/audio.wav", "rb")\ntranscript = openai.Audio.transcribe("whisper-1", audio_file)\n\nprint(transcript.text)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"3-supported-audio-formats",children:"3. Supported Audio Formats"}),"\n",(0,t.jsx)(n.p,{children:"Whisper supports the following audio formats:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"MP3"}),"\n",(0,t.jsx)(n.li,{children:"MP4"}),"\n",(0,t.jsx)(n.li,{children:"M4A"}),"\n",(0,t.jsx)(n.li,{children:"WAV"}),"\n",(0,t.jsx)(n.li,{children:"MPEG"}),"\n",(0,t.jsx)(n.li,{children:"MPGA"}),"\n",(0,t.jsx)(n.li,{children:"WEBM"}),"\n",(0,t.jsx)(n.li,{children:"FLAC"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"robotics-specific-implementation",children:"Robotics-Specific Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"1-real-time-audio-processing",children:"1. Real-time Audio Processing"}),"\n",(0,t.jsx)(n.p,{children:"For robotics applications, you'll often need to process audio in real-time. Here's an example implementation:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nimport speech_recognition as sr\nimport io\nimport wave\nimport pyaudio\n\nclass VoiceToActionProcessor:\n    def __init__(self, api_key):\n        openai.api_key = api_key\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Adjust for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n    def listen_and_transcribe(self):\n        """Listen to microphone input and transcribe using Whisper"""\n        try:\n            print("Listening for command...")\n            with self.microphone as source:\n                audio = self.recognizer.listen(source, timeout=5)\n\n            # Convert audio to WAV format for Whisper\n            wav_data = io.BytesIO()\n            with wave.open(wav_data, \'wb\') as wav_file:\n                wav_file.setnchannels(1)  # Mono\n                wav_file.setsampwidth(2)  # 16-bit\n                wav_file.setframerate(16000)  # 16kHz\n\n                # Write the audio data\n                wav_file.writeframes(audio.get_raw_data())\n\n            # Reset buffer position\n            wav_data.seek(0)\n\n            # Send to Whisper API\n            transcription = openai.Audio.transcribe(\n                model="whisper-1",\n                file=wav_data,\n                response_format="text"\n            )\n\n            return transcription.strip()\n\n        except sr.WaitTimeoutError:\n            print("No speech detected within timeout period")\n            return None\n        except Exception as e:\n            print(f"Error during transcription: {e}")\n            return None\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-batch-processing-for-better-performance",children:"2. Batch Processing for Better Performance"}),"\n",(0,t.jsx)(n.p,{children:"For applications that can batch process commands, consider this approach:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass BatchVoiceProcessor:\n    def __init__(self, api_key):\n        openai.api_key = api_key\n        self.executor = ThreadPoolExecutor(max_workers=4)\n\n    async def process_audio_batch(self, audio_files):\n        """Process multiple audio files concurrently"""\n        loop = asyncio.get_event_loop()\n\n        tasks = []\n        for audio_file in audio_files:\n            task = loop.run_in_executor(\n                self.executor,\n                self._transcribe_single_file,\n                audio_file\n            )\n            tasks.append(task)\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return results\n\n    def _transcribe_single_file(self, audio_file_path):\n        """Transcribe a single audio file"""\n        with open(audio_file_path, "rb") as file:\n            result = openai.Audio.transcribe("whisper-1", file)\n        return result\n'})}),"\n",(0,t.jsx)(n.h2,{id:"error-handling-and-best-practices",children:"Error Handling and Best Practices"}),"\n",(0,t.jsx)(n.h3,{id:"1-rate-limiting",children:"1. Rate Limiting"}),"\n",(0,t.jsx)(n.p,{children:"Whisper API has rate limits. Implement proper error handling:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import time\nimport random\n\ndef safe_transcribe_with_backoff(audio_file, max_retries=3):\n    """Transcribe with exponential backoff for rate limits"""\n    for attempt in range(max_retries):\n        try:\n            result = openai.Audio.transcribe("whisper-1", audio_file)\n            return result\n        except openai.error.RateLimitError:\n            if attempt < max_retries - 1:\n                # Exponential backoff with jitter\n                wait_time = (2 ** attempt) + random.uniform(0, 1)\n                time.sleep(wait_time)\n            else:\n                raise\n        except Exception as e:\n            print(f"Transcription failed: {e}")\n            raise\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-audio-quality-considerations",children:"2. Audio Quality Considerations"}),"\n",(0,t.jsx)(n.p,{children:"For optimal Whisper performance in robotics:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use 16kHz sample rate for audio input"}),"\n",(0,t.jsx)(n.li,{children:"Ensure clear, noise-free audio capture"}),"\n",(0,t.jsx)(n.li,{children:"Consider preprocessing audio to reduce background noise"}),"\n",(0,t.jsx)(n.li,{children:"Use directional microphones when possible"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"1-caching-common-commands",children:"1. Caching Common Commands"}),"\n",(0,t.jsx)(n.p,{children:"For frequently used commands, consider caching:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from functools import lru_cache\n\nclass OptimizedVoiceProcessor:\n    def __init__(self, api_key):\n        openai.api_key = api_key\n        self.command_cache = {}\n\n    @lru_cache(maxsize=100)\n    def _cached_transcribe(self, audio_hash):\n        """Cached transcription for repeated audio"""\n        # Implementation depends on your specific use case\n        pass\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-model-selection",children:"2. Model Selection"}),"\n",(0,t.jsx)(n.p,{children:"Whisper offers different models with trade-offs between speed and accuracy:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"whisper-1"}),": The default model, balancing speed and accuracy"]}),"\n",(0,t.jsx)(n.li,{children:"Consider using different models based on your application's requirements"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-robot-actions",children:"Integration with Robot Actions"}),"\n",(0,t.jsx)(n.p,{children:"Here's how to connect Whisper output to robot actions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class VoiceCommandRouter:\n    def __init__(self):\n        self.command_map = {\n            "move forward": "navigation/move_forward",\n            "move backward": "navigation/move_backward",\n            "turn left": "navigation/turn_left",\n            "turn right": "navigation/turn_right",\n            "pick up object": "manipulation/pick_object",\n            "place object": "manipulation/place_object",\n            "stop": "control/stop",\n            "pause": "control/pause"\n        }\n\n    def route_command(self, transcription):\n        """Map transcribed text to robot action"""\n        transcription_lower = transcription.lower()\n\n        for command, action in self.command_map.items():\n            if command in transcription_lower:\n                return action\n\n        # If no exact match, try fuzzy matching\n        return self._fuzzy_match_command(transcription_lower)\n\n    def _fuzzy_match_command(self, text):\n        """Implement fuzzy matching for partial command recognition"""\n        # Implementation of fuzzy matching algorithm\n        # This is a simplified example\n        if "forward" in text or "ahead" in text:\n            return "navigation/move_forward"\n        elif "backward" in text or "back" in text:\n            return "navigation/move_backward"\n        elif "left" in text:\n            return "navigation/turn_left"\n        elif "right" in text:\n            return "navigation/turn_right"\n        else:\n            return "unknown/command"\n'})}),"\n",(0,t.jsx)(n.h2,{id:"testing-your-integration",children:"Testing Your Integration"}),"\n",(0,t.jsx)(n.h3,{id:"1-unit-tests",children:"1. Unit Tests"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import unittest\nfrom unittest.mock import patch, MagicMock\n\nclass TestWhisperIntegration(unittest.TestCase):\n    @patch(\'openai.Audio.transcribe\')\n    def test_transcription_success(self, mock_transcribe):\n        # Mock the API response\n        mock_transcribe.return_value = MagicMock(text="move forward")\n\n        # Test your voice processing logic\n        processor = VoiceToActionProcessor("test-key")\n        result = processor._transcribe_audio("test.wav")\n\n        self.assertEqual(result, "move forward")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"2-integration-tests",children:"2. Integration Tests"}),"\n",(0,t.jsx)(n.p,{children:"Test the complete voice-to-action pipeline:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def test_voice_command_pipeline():\n    """Test complete pipeline from voice input to robot action"""\n    # This would typically involve:\n    # 1. Providing audio input (could be pre-recorded files)\n    # 2. Running through Whisper transcription\n    # 3. Mapping to robot action\n    # 4. Verifying correct action was generated\n    pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"troubleshooting-common-issues",children:"Troubleshooting Common Issues"}),"\n",(0,t.jsx)(n.h3,{id:"1-api-connection-errors",children:"1. API Connection Errors"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Verify your OpenAI API key is correct and active"}),"\n",(0,t.jsx)(n.li,{children:"Check your internet connection"}),"\n",(0,t.jsx)(n.li,{children:"Ensure you have sufficient API quota"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-poor-transcription-quality",children:"2. Poor Transcription Quality"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Check audio quality and noise levels"}),"\n",(0,t.jsx)(n.li,{children:"Verify audio format is supported"}),"\n",(0,t.jsx)(n.li,{children:"Consider preprocessing audio to improve quality"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-high-latency",children:"3. High Latency"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Optimize audio capture and processing pipeline"}),"\n",(0,t.jsx)(n.li,{children:"Consider using smaller Whisper models for faster processing"}),"\n",(0,t.jsx)(n.li,{children:"Implement caching for common commands"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"After implementing basic Whisper integration, consider:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Adding confidence scoring to transcription results"}),"\n",(0,t.jsx)(n.li,{children:"Implementing custom wake word detection"}),"\n",(0,t.jsx)(n.li,{children:"Adding support for multiple languages"}),"\n",(0,t.jsx)(n.li,{children:"Integrating with your specific robot platform (ROS 2, etc.)"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);