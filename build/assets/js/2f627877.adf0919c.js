"use strict";(globalThis.webpackChunkros2_book=globalThis.webpackChunkros2_book||[]).push([[200],{1645(e,n,t){t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>m,frontMatter:()=>c,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"vla/latency-accuracy","title":"Latency and Accuracy Considerations in Voice-to-Action Systems","description":"Overview","source":"@site/docs/vla/latency-accuracy.md","sourceDirName":"vla","slug":"/vla/latency-accuracy","permalink":"/docs/vla/latency-accuracy","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/latency-accuracy.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Voice Command Mapping Guide","permalink":"/docs/vla/voice-command-mapping"},"next":{"title":"Voice-to-Action Troubleshooting Guide","permalink":"/docs/vla/troubleshooting"}}');var s=t(4848),r=t(8453);const c={},a="Latency and Accuracy Considerations in Voice-to-Action Systems",o={},l=[{value:"Overview",id:"overview",level:2},{value:"Key Performance Metrics",id:"key-performance-metrics",level:2},{value:"1. Latency Components",id:"1-latency-components",level:3},{value:"2. Accuracy Metrics",id:"2-accuracy-metrics",level:3},{value:"Latency Optimization Strategies",id:"latency-optimization-strategies",level:2},{value:"1. Audio Stream Optimization",id:"1-audio-stream-optimization",level:3},{value:"2. Preemptive Processing",id:"2-preemptive-processing",level:3},{value:"3. Caching and Prediction",id:"3-caching-and-prediction",level:3},{value:"Accuracy Optimization Strategies",id:"accuracy-optimization-strategies",level:2},{value:"1. Confidence-Based Processing",id:"1-confidence-based-processing",level:3},{value:"2. Multi-Model Verification",id:"2-multi-model-verification",level:3},{value:"Real-Time Performance Monitoring",id:"real-time-performance-monitoring",level:2},{value:"1. Performance Tracking",id:"1-performance-tracking",level:3},{value:"2. Adaptive Processing",id:"2-adaptive-processing",level:3},{value:"Trade-off Analysis",id:"trade-off-analysis",level:2},{value:"1. Latency vs Accuracy Matrix",id:"1-latency-vs-accuracy-matrix",level:3},{value:"2. Resource Management",id:"2-resource-management",level:3},{value:"Testing Performance",id:"testing-performance",level:2},{value:"1. Load Testing",id:"1-load-testing",level:3},{value:"2. Real-World Testing",id:"2-real-world-testing",level:3},{value:"Best Practices for Production Systems",id:"best-practices-for-production-systems",level:2},{value:"Troubleshooting Performance Issues",id:"troubleshooting-performance-issues",level:2},{value:"1. High Latency Issues",id:"1-high-latency-issues",level:3},{value:"2. Low Accuracy Issues",id:"2-low-accuracy-issues",level:3},{value:"3. Resource Exhaustion",id:"3-resource-exhaustion",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"latency-and-accuracy-considerations-in-voice-to-action-systems",children:"Latency and Accuracy Considerations in Voice-to-Action Systems"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Real-time voice-to-action systems in robotics face unique challenges related to both latency (response time) and accuracy (correctness of recognition and action mapping). This guide explores the trade-offs and best practices for optimizing both aspects in robotic applications."}),"\n",(0,s.jsx)(n.h2,{id:"key-performance-metrics",children:"Key Performance Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"1-latency-components",children:"1. Latency Components"}),"\n",(0,s.jsx)(n.p,{children:"Voice-to-action systems have several latency components that add up:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Total Latency = Audio Capture + Network + Transcription + Mapping + Execution\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Capture Latency"}),": Time to capture and buffer audio input (typically 10-100ms)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Network Latency"}),": Time to send audio to cloud services (typically 50-300ms)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transcription Latency"}),": Time for speech-to-text processing (typically 200-1000ms)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mapping Latency"}),": Time to map text to actions (typically 10-50ms)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Execution Latency"}),": Time for robot to execute the action (typically 100-2000ms)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-accuracy-metrics",children:"2. Accuracy Metrics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Word Error Rate (WER)"}),": Percentage of incorrectly transcribed words"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intent Recognition Rate"}),": Percentage of correctly identified commands"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Success Rate"}),": Percentage of correctly executed robot actions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"End-to-End Success Rate"}),": Percentage of voice commands that result in correct robot behavior"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"latency-optimization-strategies",children:"Latency Optimization Strategies"}),"\n",(0,s.jsx)(n.h3,{id:"1-audio-stream-optimization",children:"1. Audio Stream Optimization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport threading\nimport queue\n\nclass OptimizedAudioCapture:\n    def __init__(self, chunk_size=1024, sample_rate=16000):\n        self.chunk_size = chunk_size\n        self.sample_rate = sample_rate\n        self.audio_queue = queue.Queue()\n        self.is_recording = False\n\n    def start_streaming_capture(self):\n        """Start audio capture with minimal latency"""\n        self.is_recording = True\n        audio = pyaudio.PyAudio()\n\n        stream = audio.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        def capture_thread():\n            while self.is_recording:\n                data = stream.read(self.chunk_size, exception_on_overflow=False)\n                self.audio_queue.put(data)\n\n        threading.Thread(target=capture_thread, daemon=True).start()\n        return stream, audio\n\n    def get_audio_chunk(self, timeout=1.0):\n        """Get audio chunk with timeout"""\n        try:\n            return self.audio_queue.get(timeout=timeout)\n        except queue.Empty:\n            return None\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-preemptive-processing",children:"2. Preemptive Processing"}),"\n",(0,s.jsx)(n.p,{children:"Process audio while the user is still speaking:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import threading\nimport time\n\nclass PreemptiveProcessor:\n    def __init__(self, whisper_client, command_mapper):\n        self.whisper_client = whisper_client\n        self.command_mapper = command_mapper\n        self.processing_queue = queue.Queue()\n        self.results_cache = {}\n\n    def start_preemptive_processing(self, audio_stream):\n        """Start processing audio chunks as they arrive"""\n        def process_chunks():\n            accumulated_audio = b""\n            chunk_count = 0\n\n            while audio_stream.is_active():\n                chunk = audio_stream.get_audio_chunk(timeout=0.5)\n                if chunk:\n                    accumulated_audio += chunk\n                    chunk_count += 1\n\n                    # Process every N chunks or when silence detected\n                    if chunk_count % 4 == 0:  # Process every 4 chunks\n                        self._process_partial_audio(accumulated_audio, chunk_count)\n\n        threading.Thread(target=process_chunks, daemon=True).start()\n\n    def _process_partial_audio(self, audio_data, chunk_count):\n        """Process partial audio for early command detection"""\n        # Convert audio data to appropriate format for Whisper\n        # This is a simplified example - real implementation would need proper audio handling\n        try:\n            # Send partial audio to Whisper for early transcription\n            # Note: Whisper typically needs complete utterances for good results\n            # This is more useful for wake word detection or simple commands\n            pass\n        except Exception as e:\n            print(f"Partial processing failed: {e}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-caching-and-prediction",children:"3. Caching and Prediction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class PredictiveCommandProcessor:\n    def __init__(self):\n        self.command_history = []\n        self.predicted_commands = {}\n        self.cache = {}\n\n    def add_to_history(self, command, result):\n        """Add command-result pair to history for prediction"""\n        self.command_history.append({\n            "command": command,\n            "result": result,\n            "timestamp": time.time()\n        })\n\n        # Keep only recent history\n        if len(self.command_history) > 100:\n            self.command_history = self.command_history[-100:]\n\n    def predict_next_commands(self):\n        """Predict likely next commands based on history"""\n        # Simple prediction based on recent commands\n        if len(self.command_history) < 2:\n            return []\n\n        # Find patterns in recent command sequences\n        recent_commands = [item["command"] for item in self.command_history[-5:]]\n\n        # Predict next likely commands (simplified logic)\n        predictions = []\n        for cmd in recent_commands:\n            if "move forward" in cmd.lower():\n                predictions.extend(["turn left", "turn right", "stop"])\n            elif "turn" in cmd.lower():\n                predictions.append("move forward")\n\n        return list(set(predictions))  # Remove duplicates\n\n    def preprocess_predictions(self):\n        """Preprocess likely commands to reduce latency"""\n        predictions = self.predict_next_commands()\n        for cmd in predictions:\n            if cmd not in self.cache:\n                # Pre-mapping common predicted commands\n                mapped = self.command_mapper.map_command(cmd)\n                self.cache[cmd] = mapped\n'})}),"\n",(0,s.jsx)(n.h2,{id:"accuracy-optimization-strategies",children:"Accuracy Optimization Strategies"}),"\n",(0,s.jsx)(n.h3,{id:"1-confidence-based-processing",children:"1. Confidence-Based Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ConfidenceBasedProcessor:\n    def __init__(self, confidence_threshold=0.8):\n        self.confidence_threshold = confidence_threshold\n        self.low_confidence_buffer = []\n\n    def process_with_confidence(self, transcription_result):\n        """Process transcription with confidence scoring"""\n        if hasattr(transcription_result, \'confidence\'):\n            confidence = transcription_result.confidence\n        else:\n            # Calculate confidence based on other factors\n            confidence = self._calculate_confidence(transcription_result)\n\n        if confidence >= self.confidence_threshold:\n            # High confidence - proceed with action\n            return self._execute_action(transcription_result, confidence)\n        else:\n            # Low confidence - ask for confirmation or buffer\n            return self._handle_low_confidence(transcription_result, confidence)\n\n    def _calculate_confidence(self, transcription):\n        """Calculate confidence based on various factors"""\n        # Length-based confidence (shorter commands might be more reliable)\n        length_factor = min(len(transcription.text) / 50, 1.0)\n\n        # Common command factor\n        common_commands = ["move forward", "move backward", "turn left", "turn right", "stop"]\n        common_factor = 1.0 if any(cmd in transcription.text.lower() for cmd in common_commands) else 0.5\n\n        # Return combined confidence score\n        return (length_factor + common_factor) / 2\n\n    def _handle_low_confidence(self, transcription, confidence):\n        """Handle low confidence transcriptions"""\n        # Add to buffer for potential follow-up questions\n        self.low_confidence_buffer.append({\n            "transcription": transcription,\n            "confidence": confidence,\n            "timestamp": time.time()\n        })\n\n        # Return request for clarification\n        return {\n            "action": "request/clarification",\n            "parameters": {\n                "original_text": transcription.text,\n                "confidence": confidence,\n                "options": self._generate_options(transcription.text)\n            }\n        }\n\n    def _generate_options(self, text):\n        """Generate possible interpretations for clarification"""\n        # Use fuzzy matching to find similar known commands\n        mapper = FuzzyCommandMapper()\n        # This would generate options based on fuzzy matching\n        return [text]  # Simplified - in practice, generate multiple options\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-multi-model-verification",children:"2. Multi-Model Verification"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class MultiModelVerification:\n    def __init__(self):\n        self.whisper_model = None  # OpenAI Whisper\n        self.local_model = None    # Local ASR model as backup\n        self.command_mapper = None\n\n    def verify_transcription(self, audio_data):\n        """Verify transcription using multiple models"""\n        # Get transcription from primary model (Whisper)\n        primary_result = self._get_whisper_transcription(audio_data)\n\n        # Get transcription from secondary model (local)\n        secondary_result = self._get_local_transcription(audio_data)\n\n        # Compare results and calculate confidence\n        similarity = self._calculate_similarity(primary_result, secondary_result)\n\n        if similarity > 0.8:\n            # High agreement - high confidence\n            return {\n                "text": primary_result,\n                "confidence": 0.9\n            }\n        elif similarity > 0.6:\n            # Medium agreement - medium confidence\n            return {\n                "text": primary_result,\n                "confidence": 0.7\n            }\n        else:\n            # Low agreement - low confidence, return both options\n            return {\n                "text": primary_result,\n                "confidence": 0.4,\n                "alternatives": [primary_result, secondary_result]\n            }\n\n    def _calculate_similarity(self, text1, text2):\n        """Calculate similarity between two transcriptions"""\n        import difflib\n        return difflib.SequenceMatcher(None, text1.lower(), text2.lower()).ratio()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"real-time-performance-monitoring",children:"Real-Time Performance Monitoring"}),"\n",(0,s.jsx)(n.h3,{id:"1-performance-tracking",children:"1. Performance Tracking"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import time\nimport statistics\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.latency_samples = []\n        self.accuracy_samples = []\n        self.start_times = {}\n\n    def start_timing(self, event_id):\n        """Start timing for a specific event"""\n        self.start_times[event_id] = time.time()\n\n    def end_timing(self, event_id):\n        """End timing and record latency"""\n        if event_id in self.start_times:\n            latency = time.time() - self.start_times[event_id]\n            self.latency_samples.append(latency)\n            del self.start_times[event_id]\n            return latency\n        return None\n\n    def record_accuracy(self, success):\n        """Record accuracy result"""\n        self.accuracy_samples.append(1 if success else 0)\n\n    def get_performance_metrics(self):\n        """Get current performance metrics"""\n        if not self.latency_samples:\n            return {"avg_latency": 0, "p95_latency": 0, "accuracy_rate": 0}\n\n        avg_latency = statistics.mean(self.latency_samples)\n        p95_latency = statistics.quantiles(self.latency_samples, n=20)[-1] if len(self.latency_samples) > 1 else avg_latency\n        accuracy_rate = statistics.mean(self.accuracy_samples) if self.accuracy_samples else 0\n\n        return {\n            "avg_latency": avg_latency,\n            "p95_latency": p95_latency,\n            "accuracy_rate": accuracy_rate,\n            "sample_count": len(self.latency_samples)\n        }\n\n    def should_optimize(self):\n        """Determine if optimization is needed based on metrics"""\n        metrics = self.get_performance_metrics()\n\n        if metrics["sample_count"] < 10:\n            return False  # Not enough data yet\n\n        # Define thresholds for optimization triggers\n        latency_threshold = 1.0  # 1 second\n        accuracy_threshold = 0.8  # 80% accuracy\n\n        return (metrics["p95_latency"] > latency_threshold or\n                metrics["accuracy_rate"] < accuracy_threshold)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-adaptive-processing",children:"2. Adaptive Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class AdaptiveProcessor:\n    def __init__(self):\n        self.monitor = PerformanceMonitor()\n        self.current_mode = "balanced"  # balanced, fast, accurate\n\n    def adapt_processing_mode(self):\n        """Adapt processing based on performance metrics"""\n        metrics = self.monitor.get_performance_metrics()\n\n        if metrics["sample_count"] < 10:\n            return  # Not enough data\n\n        if metrics["p95_latency"] > 1.5:\n            # Too slow - switch to fast mode\n            self.current_mode = "fast"\n            self._apply_fast_settings()\n        elif metrics["accuracy_rate"] < 0.75:\n            # Too inaccurate - switch to accurate mode\n            self.current_mode = "accurate"\n            self._apply_accurate_settings()\n        else:\n            # Performance is acceptable - use balanced mode\n            self.current_mode = "balanced"\n            self._apply_balanced_settings()\n\n    def _apply_fast_settings(self):\n        """Apply settings optimized for speed"""\n        # Use faster Whisper model\n        # Reduce audio quality requirements\n        # Skip some verification steps\n        pass\n\n    def _apply_accurate_settings(self):\n        """Apply settings optimized for accuracy"""\n        # Use more accurate Whisper model\n        # Increase audio quality requirements\n        # Enable verification steps\n        pass\n\n    def _apply_balanced_settings(self):\n        """Apply balanced settings"""\n        # Default settings\n        pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"trade-off-analysis",children:"Trade-off Analysis"}),"\n",(0,s.jsx)(n.h3,{id:"1-latency-vs-accuracy-matrix",children:"1. Latency vs Accuracy Matrix"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Scenario"}),(0,s.jsx)(n.th,{children:"Latency Priority"}),(0,s.jsx)(n.th,{children:"Accuracy Priority"}),(0,s.jsx)(n.th,{children:"Strategy"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Emergency stop"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Fast recognition, simple commands"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Navigation"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Balanced processing with verification"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Object manipulation"}),(0,s.jsx)(n.td,{children:"Low"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Thorough processing and confirmation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Casual interaction"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Adaptive processing"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"2-resource-management",children:"2. Resource Management"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ResourceManager:\n    def __init__(self, max_concurrent_processes=3):\n        self.max_processes = max_concurrent_processes\n        self.active_processes = 0\n        self.process_queue = queue.Queue()\n\n    def request_processing_slot(self):\n        """Request a slot for processing with rate limiting"""\n        if self.active_processes < self.max_processes:\n            self.active_processes += 1\n            return True\n        else:\n            # Queue for later processing\n            self.process_queue.put(time.time())\n            return False\n\n    def release_processing_slot(self):\n        """Release processing slot when done"""\n        if self.active_processes > 0:\n            self.active_processes -= 1\n\n        # Process queued items if slots available\n        if not self.process_queue.empty() and self.active_processes < self.max_processes:\n            self.process_queue.get()\n            self.active_processes += 1\n'})}),"\n",(0,s.jsx)(n.h2,{id:"testing-performance",children:"Testing Performance"}),"\n",(0,s.jsx)(n.h3,{id:"1-load-testing",children:"1. Load Testing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import concurrent.futures\nimport time\n\ndef test_concurrent_performance(num_clients=5, duration=60):\n    """Test system performance under concurrent load"""\n    processor = VoiceToActionProcessor(api_key="test-key")\n    results = []\n\n    def simulate_client(client_id):\n        start_time = time.time()\n        successful_commands = 0\n        total_commands = 0\n\n        while time.time() - start_time < duration:\n            # Simulate voice command\n            command = ["move forward", "turn left", "stop"][client_id % 3]\n\n            try:\n                result = processor.process_command(command)\n                if result["action"] != "unknown/command":\n                    successful_commands += 1\n                total_commands += 1\n            except Exception as e:\n                print(f"Client {client_id} error: {e}")\n\n            time.sleep(2)  # Simulate natural command spacing\n\n        results.append({\n            "client_id": client_id,\n            "success_rate": successful_commands / total_commands if total_commands > 0 else 0,\n            "total_commands": total_commands\n        })\n\n    # Run concurrent clients\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_clients) as executor:\n        futures = [executor.submit(simulate_client, i) for i in range(num_clients)]\n        concurrent.futures.wait(futures)\n\n    # Analyze results\n    avg_success_rate = sum(r["success_rate"] for r in results) / len(results)\n    print(f"Average success rate: {avg_success_rate:.2%}")\n    return results\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-real-world-testing",children:"2. Real-World Testing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def test_real_world_performance(robot_interface):\n    """Test performance with actual robot hardware"""\n    import rospy\n    from std_msgs.msg import String\n\n    # Subscribe to performance metrics from robot\n    performance_data = []\n\n    def performance_callback(data):\n        performance_data.append(rospy.get_time())\n\n    rospy.Subscriber("/robot/performance_metrics", String, performance_callback)\n\n    # Send test commands and measure end-to-end performance\n    test_commands = [\n        ("move forward 1 meter", 1.0),\n        ("turn left 90 degrees", 1.5),\n        ("stop", 0.5)\n    ]\n\n    for command, expected_time in test_commands:\n        start_time = rospy.get_time()\n\n        # Send command through voice-to-action pipeline\n        result = robot_interface.send_voice_command(command)\n\n        # Wait for expected completion time\n        time.sleep(expected_time)\n\n        end_time = rospy.get_time()\n        actual_latency = end_time - start_time\n\n        print(f"Command: {command}")\n        print(f"Expected: {expected_time}s, Actual: {actual_latency:.2f}s")\n        print(f"Success: {result[\'success\']}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices-for-production-systems",children:"Best Practices for Production Systems"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitor Continuously"}),": Implement real-time monitoring of latency and accuracy metrics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Graceful Degradation"}),": Design systems that can maintain basic functionality when performance degrades"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"User Feedback"}),": Provide clear feedback about system state and confidence levels"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fallback Mechanisms"}),": Always have backup methods for critical functions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resource Management"}),": Implement proper resource allocation and rate limiting"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Testing"}),": Regularly test with real hardware and users in real environments"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting-performance-issues",children:"Troubleshooting Performance Issues"}),"\n",(0,s.jsx)(n.h3,{id:"1-high-latency-issues",children:"1. High Latency Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Check network connectivity and bandwidth"}),"\n",(0,s.jsx)(n.li,{children:"Verify API rate limits aren't being exceeded"}),"\n",(0,s.jsx)(n.li,{children:"Optimize audio processing pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Consider edge processing for critical commands"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-low-accuracy-issues",children:"2. Low Accuracy Issues"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Improve audio quality and reduce noise"}),"\n",(0,s.jsx)(n.li,{children:"Fine-tune command mapping for domain-specific vocabulary"}),"\n",(0,s.jsx)(n.li,{children:"Implement confidence-based filtering"}),"\n",(0,s.jsx)(n.li,{children:"Add user training for optimal command phrasing"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-resource-exhaustion",children:"3. Resource Exhaustion"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement proper queuing and rate limiting"}),"\n",(0,s.jsx)(n.li,{children:"Optimize concurrent processing"}),"\n",(0,s.jsx)(n.li,{children:"Add memory management for long-running systems"}),"\n",(0,s.jsx)(n.li,{children:"Monitor system resources continuously"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"After understanding latency and accuracy considerations:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement monitoring and alerting for performance metrics"}),"\n",(0,s.jsx)(n.li,{children:"Set up automated performance testing"}),"\n",(0,s.jsx)(n.li,{children:"Create dashboards for real-time performance visualization"}),"\n",(0,s.jsx)(n.li,{children:"Establish performance baselines for your specific robot platform"}),"\n",(0,s.jsx)(n.li,{children:"Plan for performance optimization based on your use case requirements"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>c,x:()=>a});var i=t(6540);const s={},r=i.createContext(s);function c(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);