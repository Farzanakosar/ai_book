"use strict";(globalThis.webpackChunkros2_book=globalThis.webpackChunkros2_book||[]).push([[786],{2877(e,n,i){i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"vla/troubleshooting","title":"Voice-to-Action Troubleshooting Guide","description":"Overview","source":"@site/docs/vla/troubleshooting.md","sourceDirName":"vla","slug":"/vla/troubleshooting","permalink":"/docs/vla/troubleshooting","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/troubleshooting.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Latency and Accuracy Considerations in Voice-to-Action Systems","permalink":"/docs/vla/latency-accuracy"},"next":{"title":"Voice Recognition Performance Optimization Guide","permalink":"/docs/vla/performance"}}');var r=i(4848),o=i(8453);const s={},a="Voice-to-Action Troubleshooting Guide",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:2},{value:"1. Audio Capture Issues",id:"1-audio-capture-issues",level:3},{value:"Problem: &quot;No audio input detected&quot;",id:"problem-no-audio-input-detected",level:4},{value:"Problem: &quot;Poor audio quality or excessive noise&quot;",id:"problem-poor-audio-quality-or-excessive-noise",level:4},{value:"2. Whisper API Issues",id:"2-whisper-api-issues",level:3},{value:"Problem: &quot;API connection failures or timeouts&quot;",id:"problem-api-connection-failures-or-timeouts",level:4},{value:"Problem: &quot;High transcription error rates&quot;",id:"problem-high-transcription-error-rates",level:4},{value:"3. Command Mapping Issues",id:"3-command-mapping-issues",level:3},{value:"Problem: &quot;Commands not recognized or mapped incorrectly&quot;",id:"problem-commands-not-recognized-or-mapped-incorrectly",level:4},{value:"Problem: &quot;Context-aware commands not working properly&quot;",id:"problem-context-aware-commands-not-working-properly",level:4},{value:"4. Performance Issues",id:"4-performance-issues",level:3},{value:"Problem: &quot;High latency in voice-to-action pipeline&quot;",id:"problem-high-latency-in-voice-to-action-pipeline",level:4},{value:"5. Integration Issues",id:"5-integration-issues",level:3},{value:"Problem: &quot;Robot doesn&#39;t execute mapped actions&quot;",id:"problem-robot-doesnt-execute-mapped-actions",level:4},{value:"Diagnostic Tools",id:"diagnostic-tools",level:2},{value:"1. System Health Check",id:"1-system-health-check",level:3},{value:"2. Performance Monitoring",id:"2-performance-monitoring",level:3},{value:"Prevention Strategies",id:"prevention-strategies",level:2},{value:"1. Robust Error Handling",id:"1-robust-error-handling",level:3},{value:"2. Configuration Validation",id:"2-configuration-validation",level:3},{value:"Quick Reference",id:"quick-reference",level:2},{value:"Common Error Messages and Solutions",id:"common-error-messages-and-solutions",level:3},{value:"Performance Optimization Checklist",id:"performance-optimization-checklist",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"voice-to-action-troubleshooting-guide",children:"Voice-to-Action Troubleshooting Guide"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This guide provides solutions to common issues encountered when implementing and using voice-to-action pipelines with OpenAI Whisper in robotics applications. It covers problems related to audio capture, transcription, command mapping, and robot execution."}),"\n",(0,r.jsx)(n.h2,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,r.jsx)(n.h3,{id:"1-audio-capture-issues",children:"1. Audio Capture Issues"}),"\n",(0,r.jsx)(n.h4,{id:"problem-no-audio-input-detected",children:'Problem: "No audio input detected"'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"System doesn't respond to voice commands"}),"\n",(0,r.jsx)(n.li,{children:"Microphone indicator shows no activity"}),"\n",(0,r.jsx)(n.li,{children:"Error messages about audio device access"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Check hardware connections:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# On Linux, check available audio devices\narecord -l\n\n# On Windows, check Device Manager for audio devices\n# On macOS, check System Preferences > Sound\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Verify microphone permissions:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Windows: Settings > Privacy > Microphone"}),"\n",(0,r.jsx)(n.li,{children:"macOS: System Preferences > Security & Privacy > Privacy > Microphone"}),"\n",(0,r.jsx)(n.li,{children:"Linux: Check if running in a container that has audio access"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Test audio input directly:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport wave\n\ndef test_microphone():\n    CHUNK = 1024\n    FORMAT = pyaudio.paInt16\n    CHANNELS = 1\n    RATE = 44100\n    RECORD_SECONDS = 5\n    WAVE_OUTPUT_FILENAME = "test.wav"\n\n    p = pyaudio.PyAudio()\n\n    stream = p.open(format=FORMAT,\n                    channels=CHANNELS,\n                    rate=RATE,\n                    input=True,\n                    frames_per_buffer=CHUNK)\n\n    print("Recording...")\n    frames = []\n\n    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n        data = stream.read(CHUNK)\n        frames.append(data)\n\n    print("Done recording.")\n\n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n    wf = wave.open(WAVE_OUTPUT_FILENAME, \'wb\')\n    wf.setnchannels(CHANNELS)\n    wf.setsampwidth(p.get_sample_size(FORMAT))\n    wf.setframerate(RATE)\n    wf.writeframes(b\'\'.join(frames))\n    wf.close()\n\ntest_microphone()\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"problem-poor-audio-quality-or-excessive-noise",children:'Problem: "Poor audio quality or excessive noise"'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"High Word Error Rate (WER) in transcriptions"}),"\n",(0,r.jsx)(n.li,{children:"Commands frequently misunderstood"}),"\n",(0,r.jsx)(n.li,{children:"Background noise interference"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Optimize audio settings:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import speech_recognition as sr\n\ndef optimize_audio_settings():\n    r = sr.Recognizer()\n    m = sr.Microphone()\n\n    # Adjust for ambient noise\n    with m as source:\n        r.adjust_for_ambient_noise(source, duration=1.0)\n\n    # Set energy threshold (default is 300)\n    r.energy_threshold = 4000  # Increase for noisy environments\n\n    # Set dynamic energy threshold\n    r.dynamic_energy_threshold = True\n\n    return r, m\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Use noise reduction techniques:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy import signal\n\ndef reduce_noise(audio_data, sample_rate=16000):\n    """Apply basic noise reduction to audio data"""\n    # Convert bytes to numpy array\n    audio_array = np.frombuffer(audio_data, dtype=np.int16)\n\n    # Apply a simple low-pass filter to remove high-frequency noise\n    nyquist = sample_rate / 2\n    cutoff = 0.8 * nyquist  # 80% of nyquist frequency\n\n    # Design and apply filter\n    b, a = signal.butter(4, cutoff / nyquist, btype=\'low\')\n    filtered_audio = signal.filtfilt(b, a, audio_array)\n\n    # Convert back to bytes\n    return filtered_audio.astype(np.int16).tobytes()\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-whisper-api-issues",children:"2. Whisper API Issues"}),"\n",(0,r.jsx)(n.h4,{id:"problem-api-connection-failures-or-timeouts",children:'Problem: "API connection failures or timeouts"'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Timeout errors when calling Whisper API"}),"\n",(0,r.jsx)(n.li,{children:'"Connection refused" or "Network error" messages'}),"\n",(0,r.jsx)(n.li,{children:"Slow response times"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Check network connectivity:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Test connectivity to OpenAI\ncurl -I https://api.openai.com/v1/models\n"})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Verify API key:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import openai\n\ndef test_api_key():\n    try:\n        # Test with a simple API call\n        models = openai.Model.list()\n        print("API key is valid")\n        return True\n    except openai.error.AuthenticationError:\n        print("Invalid API key")\n        return False\n    except Exception as e:\n        print(f"API test failed: {e}")\n        return False\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Implement retry logic:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nimport random\nfrom functools import wraps\n\ndef retry_with_backoff(max_retries=3, base_delay=1, max_delay=60):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            for attempt in range(max_retries):\n                try:\n                    return func(*args, **kwargs)\n                except openai.error.RateLimitError:\n                    if attempt < max_retries - 1:\n                        delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)\n                        time.sleep(delay)\n                        continue\n                    raise\n                except openai.error.APIConnectionError:\n                    if attempt < max_retries - 1:\n                        delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)\n                        time.sleep(delay)\n                        continue\n                    raise\n                except Exception:\n                    if attempt < max_retries - 1:\n                        time.sleep(1)\n                        continue\n                    raise\n            return None\n        return wrapper\n    return decorator\n\n@retry_with_backoff(max_retries=3)\ndef transcribe_with_retry(audio_file):\n    return openai.Audio.transcribe("whisper-1", audio_file)\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"problem-high-transcription-error-rates",children:'Problem: "High transcription error rates"'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Commands frequently misrecognized"}),"\n",(0,r.jsx)(n.li,{children:"Inconsistent transcription results"}),"\n",(0,r.jsx)(n.li,{children:"Poor understanding of specific terminology"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Optimize audio format for Whisper:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import io\nimport wave\n\ndef prepare_audio_for_whisper(raw_audio, sample_rate=16000):\n    """Prepare audio in optimal format for Whisper API"""\n    wav_buffer = io.BytesIO()\n\n    with wave.open(wav_buffer, \'wb\') as wav_file:\n        wav_file.setnchannels(1)  # Mono\n        wav_file.setsampwidth(2)  # 16-bit\n        wav_file.setframerate(sample_rate)  # 16kHz\n        wav_file.writeframes(raw_audio)\n\n    wav_buffer.seek(0)\n    return wav_buffer\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Preprocess audio for better results:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def enhance_audio_for_transcription(audio_data):\n    """Enhance audio quality before sending to Whisper"""\n    # Convert to numpy array\n    audio_array = np.frombuffer(audio_data, dtype=np.int16)\n\n    # Normalize audio levels (Whisper works best with normalized audio)\n    max_amplitude = np.max(np.abs(audio_array))\n    if max_amplitude > 0:\n        # Normalize to reasonable level\n        normalization_factor = 0.8 * (2**15) / max_amplitude\n        audio_array = audio_array * normalization_factor\n\n    # Apply gentle compression to improve clarity\n    # This is a simple implementation - consider using proper audio processing libraries\n    audio_array = np.clip(audio_array, -0.9 * (2**15), 0.9 * (2**15))\n\n    # Convert back to bytes\n    return audio_array.astype(np.int16).tobytes()\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-command-mapping-issues",children:"3. Command Mapping Issues"}),"\n",(0,r.jsx)(n.h4,{id:"problem-commands-not-recognized-or-mapped-incorrectly",children:'Problem: "Commands not recognized or mapped incorrectly"'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Voice commands result in "unknown" actions'}),"\n",(0,r.jsx)(n.li,{children:"Similar commands mapped to different actions inconsistently"}),"\n",(0,r.jsx)(n.li,{children:"Expected actions not being triggered"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Debug command mapping:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def debug_command_mapping(command_text, mapper):\n    """Debug the command mapping process"""\n    print(f"Input command: \'{command_text}\'")\n    print(f"Lowercase: \'{command_text.lower()}\'")\n\n    # Test each pattern\n    for i, pattern_config in enumerate(mapper.command_patterns):\n        match = re.search(pattern_config["pattern"], command_text.lower())\n        if match:\n            print(f"Pattern {i} matched: {pattern_config[\'pattern\']}")\n            print(f"Groups: {match.groups()}")\n            break\n    else:\n        print("No patterns matched")\n\n    # Test fuzzy matching\n    fuzzy_result = mapper._fuzzy_match_command(command_text.lower())\n    print(f"Fuzzy match result: {fuzzy_result.action_type}")\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Improve pattern matching:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ImprovedCommandMapper(AdvancedCommandMapper):\n    def __init__(self):\n        super().__init__()\n        # Add more flexible patterns\n        self.command_patterns.extend([\n            # More flexible navigation patterns\n            {\n                "pattern": r"(?:move|go|step)\\s+(forward|backward|ahead|back)\\s*(?:by\\s+)?(\\d+(?:\\.\\d+)?)?",\n                "action_type": "navigation/move_distance",\n                "extractors": [\n                    lambda m: ("direction", "forward" if m.group(1) in ["forward", "ahead"] else "backward"),\n                    lambda m: ("distance", float(m.group(2)) if m.group(2) else 1.0)\n                ]\n            },\n            # Flexible manipulation patterns\n            {\n                "pattern": r"(?:pick|take|grab)\\s+(?:up\\s+)?(?:the\\s+)?(\\w+)\\s*(?:from\\s+(?:the\\s+)?(\\w+))?",\n                "action_type": "manipulation/pick_object",\n                "extractors": [\n                    lambda m: ("object_type", m.group(1)),\n                    lambda m: ("source_location", m.group(2)) if m.group(2) else ("source_location", "unknown")\n                ]\n            }\n        ])\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"problem-context-aware-commands-not-working-properly",children:'Problem: "Context-aware commands not working properly"'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Context information not being used in command processing"}),"\n",(0,r.jsx)(n.li,{children:"Same commands producing different results in different contexts"}),"\n",(0,r.jsx)(n.li,{children:"Context state not being maintained properly"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Verify context management:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ContextManager:\n    def __init__(self):\n        self.context = {}\n        self.history = []\n\n    def update_context(self, new_context):\n        """Update context with new information"""\n        self.context.update(new_context)\n        self.history.append({\n            "timestamp": time.time(),\n            "context": new_context.copy()\n        })\n\n    def get_relevant_context(self, max_age=300):  # 5 minutes\n        """Get context that\'s not too old"""\n        current_time = time.time()\n        relevant_context = {}\n\n        # Include recent context\n        for item in reversed(self.history):\n            if current_time - item["timestamp"] <= max_age:\n                relevant_context.update(item["context"])\n            else:\n                break\n\n        return relevant_context\n\n    def clear_old_context(self, max_age=300):\n        """Remove old context history"""\n        current_time = time.time()\n        self.history = [item for item in self.history\n                       if current_time - item["timestamp"] <= max_age]\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"4-performance-issues",children:"4. Performance Issues"}),"\n",(0,r.jsx)(n.h4,{id:"problem-high-latency-in-voice-to-action-pipeline",children:'Problem: "High latency in voice-to-action pipeline"'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Long delay between speaking command and robot response"}),"\n",(0,r.jsx)(n.li,{children:"Timeout errors in real-time applications"}),"\n",(0,r.jsx)(n.li,{children:"Poor user experience due to delays"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Optimize the pipeline:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport concurrent.futures\nfrom threading import Thread\n\nclass OptimizedVoiceProcessor:\n    def __init__(self, api_key):\n        openai.api_key = api_key\n        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=2)\n        self.cache = {}\n        self.cache_size_limit = 50\n\n    def process_voice_command(self, audio_data):\n        """Process voice command with optimization"""\n        # Preprocessing in background\n        processed_audio = self._preprocess_audio(audio_data)\n\n        # Check cache first\n        audio_hash = hash(processed_audio)\n        if audio_hash in self.cache:\n            return self.cache[audio_hash]\n\n        # Submit to executor for non-blocking processing\n        future = self.executor.submit(self._process_audio, processed_audio)\n        result = future.result(timeout=10.0)  # 10 second timeout\n\n        # Update cache\n        self._update_cache(audio_hash, result)\n\n        return result\n\n    def _update_cache(self, audio_hash, result):\n        """Update cache with new result"""\n        if len(self.cache) >= self.cache_size_limit:\n            # Remove oldest entry\n            oldest_key = next(iter(self.cache))\n            del self.cache[oldest_key]\n\n        self.cache[audio_hash] = result\n'})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Implement streaming or partial processing:"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class StreamingVoiceProcessor:\n    def __init__(self, api_key):\n        openai.api_key = api_key\n        self.audio_buffer = b""\n        self.min_audio_length = 8000  # Minimum bytes before processing\n\n    def add_audio_chunk(self, chunk):\n        """Add audio chunk to buffer"""\n        self.audio_buffer += chunk\n\n        # Process if we have enough audio\n        if len(self.audio_buffer) >= self.min_audio_length:\n            # Check if we have a potential end-of-sentence\n            if self._likely_end_of_command():\n                self._process_buffered_audio()\n                self.audio_buffer = b""  # Reset buffer\n\n    def _likely_end_of_command(self):\n        """Check if audio likely contains complete command"""\n        # Simple heuristic - look for periods of silence\n        # In practice, use more sophisticated audio analysis\n        silence_threshold = 100  # Adjust based on your audio format\n        audio_array = np.frombuffer(self.audio_buffer[-1000:], dtype=np.int16)\n        return np.mean(np.abs(audio_array)) < silence_threshold\n\n    def _process_buffered_audio(self):\n        """Process the buffered audio"""\n        if len(self.audio_buffer) < self.min_audio_length:\n            return\n\n        # Process the audio\n        # This would call Whisper and command mapping\n        pass\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"5-integration-issues",children:"5. Integration Issues"}),"\n",(0,r.jsx)(n.h4,{id:"problem-robot-doesnt-execute-mapped-actions",children:'Problem: "Robot doesn\'t execute mapped actions"'}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Symptoms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Commands properly mapped but robot doesn't respond"}),"\n",(0,r.jsx)(n.li,{children:"Action execution fails silently"}),"\n",(0,r.jsx)(n.li,{children:"Robot in wrong state for action execution"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Solutions:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement action execution verification:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ActionExecutor:\n    def __init__(self, robot_interface):\n        self.robot = robot_interface\n\n    def execute_action(self, robot_action):\n        """Execute robot action with verification"""\n        try:\n            print(f"Executing action: {robot_action.action_type}")\n            print(f"Parameters: {robot_action.parameters}")\n\n            # Verify robot is in correct state\n            if not self._verify_robot_state(robot_action):\n                raise ValueError(f"Robot not in appropriate state for {robot_action.action_type}")\n\n            # Execute the action\n            result = self._execute_by_type(robot_action)\n\n            # Verify execution was successful\n            if not self._verify_execution_success(robot_action):\n                raise RuntimeError("Action execution failed verification")\n\n            print(f"Action completed successfully: {robot_action.action_type}")\n            return True\n\n        except Exception as e:\n            print(f"Action execution failed: {e}")\n            return False\n\n    def _verify_robot_state(self, robot_action):\n        """Verify robot is in appropriate state for action"""\n        current_state = self.robot.get_state()\n\n        # Define state requirements for different action types\n        state_requirements = {\n            "navigation/move_forward": ["idle", "ready"],\n            "manipulation/pick_object": ["idle", "ready", "navigated"],\n            "control/stop": ["*"]  # Any state\n        }\n\n        required_states = state_requirements.get(robot_action.action_type, ["idle"])\n        return required_states[0] == "*" or current_state in required_states\n\n    def _execute_by_type(self, robot_action):\n        """Execute action based on type"""\n        action_type = robot_action.action_type\n\n        if action_type.startswith("navigation/"):\n            return self._execute_navigation(robot_action)\n        elif action_type.startswith("manipulation/"):\n            return self._execute_manipulation(robot_action)\n        elif action_type.startswith("control/"):\n            return self._execute_control(robot_action)\n        else:\n            raise ValueError(f"Unknown action type: {action_type}")\n\n    def _verify_execution_success(self, robot_action):\n        """Verify that action was executed successfully"""\n        # Implementation depends on your robot platform\n        # This is a placeholder\n        time.sleep(0.1)  # Simulate execution time\n        return True\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"diagnostic-tools",children:"Diagnostic Tools"}),"\n",(0,r.jsx)(n.h3,{id:"1-system-health-check",children:"1. System Health Check"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def run_system_health_check():\n    """Run comprehensive system health check"""\n    results = {\n        "audio_system": False,\n        "api_connection": False,\n        "command_mapping": False,\n        "robot_interface": False\n    }\n\n    # Check audio system\n    try:\n        import pyaudio\n        p = pyaudio.PyAudio()\n        device_count = p.get_device_count()\n        p.terminate()\n        results["audio_system"] = device_count > 0\n        print(f"Audio devices: {device_count}")\n    except Exception as e:\n        print(f"Audio system check failed: {e}")\n\n    # Check API connection\n    try:\n        import openai\n        models = openai.Model.list()\n        results["api_connection"] = True\n        print("API connection: OK")\n    except Exception as e:\n        print(f"API connection failed: {e}")\n\n    # Check command mapping\n    try:\n        mapper = AdvancedCommandMapper()\n        test_action = mapper.map_command("move forward")\n        results["command_mapping"] = test_action.action_type != "unknown/command"\n        print(f"Command mapping: {\'OK\' if results[\'command_mapping\'] else \'FAILED\'}")\n    except Exception as e:\n        print(f"Command mapping check failed: {e}")\n\n    # Check robot interface (if available)\n    # This would depend on your specific robot platform\n    results["robot_interface"] = True  # Placeholder\n    print("Robot interface: Assuming OK (implementation specific)")\n\n    return results\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-performance-monitoring",children:"2. Performance Monitoring"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\nimport statistics\n\nclass PerformanceMonitor:\n    def __init__(self):\n        self.metrics = {\n            "transcription_times": [],\n            "command_mapping_times": [],\n            "total_processing_times": [],\n            "success_counts": {"transcription": 0, "mapping": 0, "execution": 0},\n            "error_counts": {"transcription": 0, "mapping": 0, "execution": 0}\n        }\n\n    def start_timer(self, operation):\n        """Start timing for an operation"""\n        self.metrics[f"{operation}_start"] = time.time()\n\n    def end_timer(self, operation):\n        """End timing for an operation"""\n        start_time = self.metrics.get(f"{operation}_start")\n        if start_time:\n            elapsed = time.time() - start_time\n            self.metrics[f"{operation}_times"].append(elapsed)\n            del self.metrics[f"{operation}_start"]\n            return elapsed\n        return None\n\n    def record_success(self, operation):\n        """Record successful operation"""\n        self.metrics["success_counts"][operation] += 1\n\n    def record_error(self, operation):\n        """Record failed operation"""\n        self.metrics["error_counts"][operation] += 1\n\n    def get_performance_report(self):\n        """Generate performance report"""\n        report = {\n            "transcription": {},\n            "command_mapping": {},\n            "total_processing": {}\n        }\n\n        for op in ["transcription", "command_mapping", "total_processing"]:\n            times = self.metrics[f"{op}_times"]\n            if times:\n                report[op] = {\n                    "avg_time": statistics.mean(times),\n                    "p95_time": statistics.quantiles(times, n=20)[-1] if len(times) > 1 else times[0],\n                    "min_time": min(times),\n                    "max_time": max(times),\n                    "count": len(times)\n                }\n            else:\n                report[op] = {"avg_time": 0, "count": 0}\n\n        report["success_rates"] = {}\n        for op in ["transcription", "mapping", "execution"]:\n            total = self.metrics["success_counts"][op] + self.metrics["error_counts"][op]\n            if total > 0:\n                report["success_rates"][op] = self.metrics["success_counts"][op] / total\n            else:\n                report["success_rates"][op] = 0\n\n        return report\n'})}),"\n",(0,r.jsx)(n.h2,{id:"prevention-strategies",children:"Prevention Strategies"}),"\n",(0,r.jsx)(n.h3,{id:"1-robust-error-handling",children:"1. Robust Error Handling"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def robust_voice_processing(audio_data, fallback_timeout=5.0):\n    """Process voice with multiple fallback strategies"""\n    results = []\n\n    # Strategy 1: Standard processing\n    try:\n        result = standard_voice_processor(audio_data)\n        results.append(("standard", result, "success"))\n    except Exception as e:\n        results.append(("standard", None, str(e)))\n\n    # Strategy 2: Fallback processing (simpler, more robust)\n    try:\n        result = fallback_voice_processor(audio_data)\n        results.append(("fallback", result, "success"))\n    except Exception as e:\n        results.append(("fallback", None, str(e)))\n\n    # Strategy 3: Local processing if available\n    try:\n        result = local_voice_processor(audio_data)\n        results.append(("local", result, "success"))\n    except Exception as e:\n        results.append(("local", None, str(e)))\n\n    # Return best available result\n    for strategy, result, status in results:\n        if result and status == "success":\n            return result\n\n    # If all failed, return error information\n    return {"error": "All processing strategies failed", "details": results}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"2-configuration-validation",children:"2. Configuration Validation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'def validate_configuration(config):\n    """Validate system configuration before starting"""\n    errors = []\n\n    # Check API key\n    if not config.get("openai_api_key"):\n        errors.append("OpenAI API key is missing")\n\n    # Check audio device\n    try:\n        import pyaudio\n        p = pyaudio.PyAudio()\n        if p.get_device_count() == 0:\n            errors.append("No audio devices found")\n        p.terminate()\n    except Exception as e:\n        errors.append(f"Audio system error: {e}")\n\n    # Check required packages\n    required_packages = ["openai", "speechrecognition", "pyaudio"]\n    for package in required_packages:\n        try:\n            __import__(package)\n        except ImportError:\n            errors.append(f"Missing required package: {package}")\n\n    return {\n        "valid": len(errors) == 0,\n        "errors": errors\n    }\n'})}),"\n",(0,r.jsx)(n.h2,{id:"quick-reference",children:"Quick Reference"}),"\n",(0,r.jsx)(n.h3,{id:"common-error-messages-and-solutions",children:"Common Error Messages and Solutions"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Error Message"}),(0,r.jsx)(n.th,{children:"Likely Cause"}),(0,r.jsx)(n.th,{children:"Solution"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:'"No speech detected"'}),(0,r.jsx)(n.td,{children:"Microphone not working or environment too quiet"}),(0,r.jsx)(n.td,{children:"Check microphone connections, increase energy threshold"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:'"Invalid API key"'}),(0,r.jsx)(n.td,{children:"Incorrect or expired API key"}),(0,r.jsx)(n.td,{children:"Verify API key in environment variables"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:'"Rate limit exceeded"'}),(0,r.jsx)(n.td,{children:"Too many API requests"}),(0,r.jsx)(n.td,{children:"Implement rate limiting and retry logic"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:'"Connection timeout"'}),(0,r.jsx)(n.td,{children:"Network connectivity issues"}),(0,r.jsx)(n.td,{children:"Check internet connection, increase timeout values"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:'"Unknown command"'}),(0,r.jsx)(n.td,{children:"Command not in pattern list"}),(0,r.jsx)(n.td,{children:"Add command pattern or improve fuzzy matching"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:'"Action failed"'}),(0,r.jsx)(n.td,{children:"Robot state or execution error"}),(0,r.jsx)(n.td,{children:"Check robot state, verify action parameters"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"performance-optimization-checklist",children:"Performance Optimization Checklist"}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement audio preprocessing"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Add caching for repeated commands"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Use connection pooling for API calls"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Implement asynchronous processing"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Monitor and log performance metrics"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set appropriate timeout values"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Validate inputs before processing"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsx)(n.p,{children:"If you're still experiencing issues after following this guide:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Check the ",(0,r.jsx)(n.a,{href:"/docs/vla/performance",children:"Performance Optimization Guide"})," for advanced optimization techniques"]}),"\n",(0,r.jsx)(n.li,{children:"Review your specific robot platform documentation for integration issues"}),"\n",(0,r.jsx)(n.li,{children:"Consider implementing more sophisticated error recovery mechanisms"}),"\n",(0,r.jsx)(n.li,{children:"Set up monitoring and alerting for production systems"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const r={},o=t.createContext(r);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);