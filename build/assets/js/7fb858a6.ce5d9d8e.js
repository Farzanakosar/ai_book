"use strict";(globalThis.webpackChunkros2_book=globalThis.webpackChunkros2_book||[]).push([[612],{2479(n,e,t){t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"vla/exercises","title":"Voice-to-Action Exercises","description":"Exercise 1: Basic Voice Command Recognition","source":"@site/docs/vla/exercises.md","sourceDirName":"vla","slug":"/vla/exercises","permalink":"/docs/vla/exercises","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/vla/exercises.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Voice Recognition Performance Optimization Guide","permalink":"/docs/vla/performance"},"next":{"title":"Validation of User Story 1 Content Against OpenAI Documentation Standards","permalink":"/docs/vla/validation-report"}}');var r=t(4848),a=t(8453);const o={},s="Voice-to-Action Exercises",c={},d=[{value:"Exercise 1: Basic Voice Command Recognition",id:"exercise-1-basic-voice-command-recognition",level:2},{value:"Objective",id:"objective",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Instructions",id:"instructions",level:3},{value:"Exercise Tasks",id:"exercise-tasks",level:3},{value:"Expected Output",id:"expected-output",level:3},{value:"Solution Hints",id:"solution-hints",level:3},{value:"Exercise 2: Whisper Integration Exercise",id:"exercise-2-whisper-integration-exercise",level:2},{value:"Objective",id:"objective-1",level:3},{value:"Prerequisites",id:"prerequisites-1",level:3},{value:"Instructions",id:"instructions-1",level:3},{value:"Exercise Tasks",id:"exercise-tasks-1",level:3},{value:"Expected Output",id:"expected-output-1",level:3},{value:"Advanced Challenge",id:"advanced-challenge",level:3},{value:"Exercise 3: Command Mapping Exercise",id:"exercise-3-command-mapping-exercise",level:2},{value:"Objective",id:"objective-2",level:3},{value:"Prerequisites",id:"prerequisites-2",level:3},{value:"Instructions",id:"instructions-2",level:3},{value:"Exercise Tasks",id:"exercise-tasks-2",level:3},{value:"Expected Output",id:"expected-output-2",level:3},{value:"Advanced Challenge",id:"advanced-challenge-1",level:3},{value:"Exercise 4: Performance Optimization and Troubleshooting",id:"exercise-4-performance-optimization-and-troubleshooting",level:2},{value:"Objective",id:"objective-3",level:3},{value:"Instructions",id:"instructions-3",level:3}];function l(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"voice-to-action-exercises",children:"Voice-to-Action Exercises"})}),"\n",(0,r.jsx)(e.h2,{id:"exercise-1-basic-voice-command-recognition",children:"Exercise 1: Basic Voice Command Recognition"}),"\n",(0,r.jsx)(e.h3,{id:"objective",children:"Objective"}),"\n",(0,r.jsx)(e.p,{children:"Implement a basic voice command system that can recognize simple commands and map them to actions."}),"\n",(0,r.jsx)(e.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"OpenAI API key configured"}),"\n",(0,r.jsx)(e.li,{children:"Python environment with required packages installed"}),"\n",(0,r.jsx)(e.li,{children:"Working microphone"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"instructions",children:"Instructions"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Set up the basic voice processing system:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import openai\nimport speech_recognition as sr\nimport os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\nopenai.api_key = os.getenv("OPENAI_API_KEY")\n\n# Initialize speech recognizer\nrecognizer = sr.Recognizer()\nmicrophone = sr.Microphone()\n\n# Adjust for ambient noise\nwith microphone as source:\n    recognizer.adjust_for_ambient_noise(source)\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Create a simple voice command function:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def listen_for_command():\n    """Listen for a voice command and return the transcribed text"""\n    print("Listening for command...")\n\n    with microphone as source:\n        audio = recognizer.listen(source, timeout=5)\n\n    try:\n        # Use Whisper API for transcription\n        import io\n        import wave\n\n        # Convert audio to appropriate format for Whisper\n        wav_data = io.BytesIO()\n        with wave.open(wav_data, \'wb\') as wav_file:\n            wav_file.setnchannels(1)  # Mono\n            wav_file.setsampwidth(2)  # 16-bit\n            wav_file.setframerate(16000)  # 16kHz\n\n            # Get raw audio data and write to buffer\n            raw_data = audio.get_raw_data()\n            wav_file.writeframes(raw_data)\n\n        wav_data.seek(0)\n\n        transcription = openai.Audio.transcribe("whisper-1", wav_data)\n        return transcription.text\n\n    except sr.WaitTimeoutError:\n        print("No speech detected within timeout period")\n        return None\n    except Exception as e:\n        print(f"Error during transcription: {e}")\n        return None\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Create a simple command mapping function:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def map_command_to_action(command_text):\n    """Map recognized command to robot action"""\n    if command_text is None:\n        return "unknown"\n\n    command_lower = command_text.lower()\n\n    if "move forward" in command_lower or "go forward" in command_lower:\n        return "navigation/move_forward"\n    elif "move backward" in command_lower or "go backward" in command_lower:\n        return "navigation/move_backward"\n    elif "turn left" in command_lower:\n        return "navigation/turn_left"\n    elif "turn right" in command_lower:\n        return "navigation/turn_right"\n    elif "stop" in command_lower:\n        return "control/stop"\n    else:\n        return "unknown"\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Create the main loop to test the system:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def main():\n    print("Voice Command System - Ready")\n    print("Say commands like \'move forward\', \'turn left\', \'stop\'")\n    print("Press Ctrl+C to exit")\n\n    try:\n        while True:\n            command_text = listen_for_command()\n\n            if command_text:\n                print(f"Heard: {command_text}")\n                action = map_command_to_action(command_text)\n                print(f"Mapped to action: {action}")\n\n                # In a real robot system, you would execute the action here\n                # For this exercise, we just print it\n\n                print("-" * 40)\n            else:\n                print("No command recognized, listening again...")\n\n    except KeyboardInterrupt:\n        print("\\nExiting voice command system...")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"exercise-tasks",children:"Exercise Tasks"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Implementation:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Create a new Python file called ",(0,r.jsx)(e.code,{children:"voice_exercise.py"})]}),"\n",(0,r.jsx)(e.li,{children:"Implement the code above to create a basic voice command system"}),"\n",(0,r.jsx)(e.li,{children:"Test that the system can capture your voice and send it to Whisper"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Extension:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'Add at least 3 more commands to the mapping function (e.g., "pick up object", "place object", "turn around")'}),"\n",(0,r.jsx)(e.li,{children:"Add error handling for API rate limits"}),"\n",(0,r.jsx)(e.li,{children:"Implement a simple confidence score display"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Testing:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Test your system with different commands"}),"\n",(0,r.jsx)(e.li,{children:"Try the same command multiple times to see consistency"}),"\n",(0,r.jsx)(e.li,{children:"Test in different noise environments"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"expected-output",children:"Expected Output"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Voice Command System - Ready\nSay commands like 'move forward', 'turn left', 'stop'\nPress Ctrl+C to exit\nListening for command...\nHeard: move forward\nMapped to action: navigation/move_forward\n----------------------------------------\nListening for command...\nHeard: turn left\nMapped to action: navigation/turn_left\n----------------------------------------\n"})}),"\n",(0,r.jsx)(e.h3,{id:"solution-hints",children:"Solution Hints"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Make sure your microphone is working and permissions are granted"}),"\n",(0,r.jsx)(e.li,{children:"Check that your OpenAI API key is valid and has sufficient credits"}),"\n",(0,r.jsx)(e.li,{children:"If getting rate limit errors, add delays between requests or implement retry logic"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"exercise-2-whisper-integration-exercise",children:"Exercise 2: Whisper Integration Exercise"}),"\n",(0,r.jsx)(e.h3,{id:"objective-1",children:"Objective"}),"\n",(0,r.jsx)(e.p,{children:"Integrate OpenAI Whisper API for speech-to-text conversion and optimize the integration for robotics applications."}),"\n",(0,r.jsx)(e.h3,{id:"prerequisites-1",children:"Prerequisites"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Basic voice command system from Exercise 1"}),"\n",(0,r.jsx)(e.li,{children:"OpenAI API key with Whisper access"}),"\n",(0,r.jsx)(e.li,{children:"Understanding of audio formats and processing"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"instructions-1",children:"Instructions"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Enhanced Whisper Integration:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import openai\nimport io\nimport wave\nimport time\nfrom typing import Optional, Dict, Any\n\nclass WhisperIntegrator:\n    def __init__(self, api_key: str, model: str = "whisper-1"):\n        openai.api_key = api_key\n        self.model = model\n        self.transcription_history = []\n\n    def transcribe_audio_buffer(self, audio_data: bytes,\n                              sample_rate: int = 16000,\n                              channels: int = 1) -> Optional[str]:\n        """Transcribe raw audio bytes using Whisper API"""\n        try:\n            # Create WAV buffer from raw audio data\n            wav_buffer = io.BytesIO()\n\n            with wave.open(wav_buffer, \'wb\') as wav_file:\n                wav_file.setnchannels(channels)\n                wav_file.setsampwidth(2)  # 16-bit\n                wav_file.setframerate(sample_rate)\n                wav_file.writeframes(audio_data)\n\n            # Reset buffer position\n            wav_buffer.seek(0)\n\n            # Transcribe using Whisper\n            start_time = time.time()\n            result = openai.Audio.transcribe(\n                model=self.model,\n                file=wav_buffer,\n                response_format="text"\n            )\n\n            # Record performance metrics\n            transcription_time = time.time() - start_time\n            self.transcription_history.append({\n                "text": result,\n                "processing_time": transcription_time,\n                "timestamp": time.time()\n            })\n\n            return result\n\n        except Exception as e:\n            print(f"Whisper transcription error: {e}")\n            return None\n\n    def transcribe_with_timing_options(self, audio_data: bytes) -> Dict[str, Any]:\n        """Transcribe with additional options for timing control"""\n        try:\n            wav_buffer = io.BytesIO()\n\n            with wave.open(wav_buffer, \'wb\') as wav_file:\n                wav_file.setnchannels(1)\n                wav_file.setsampwidth(2)\n                wav_file.setframerate(16000)\n                wav_file.writeframes(audio_data)\n\n            wav_buffer.seek(0)\n\n            # Get detailed response with timing info\n            result = openai.Audio.transcribe(\n                model=self.model,\n                file=wav_buffer,\n                response_format="verbose_json",  # More detailed response\n                timestamp_granularities=["segment"]\n            )\n\n            return {\n                "text": result.text,\n                "segments": result.segments,\n                "processing_time": result.duration\n            }\n\n        except Exception as e:\n            print(f"Detailed transcription error: {e}")\n            return {"text": None, "segments": [], "processing_time": 0}\n\n    def get_performance_metrics(self) -> Dict[str, float]:\n        """Get performance metrics for Whisper integration"""\n        if not self.transcription_history:\n            return {"avg_time": 0, "count": 0}\n\n        times = [item["processing_time"] for item in self.transcription_history]\n        avg_time = sum(times) / len(times)\n\n        return {\n            "avg_time": avg_time,\n            "count": len(times),\n            "min_time": min(times),\n            "max_time": max(times)\n        }\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Audio Preprocessing for Better Whisper Results:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy import signal\n\nclass AudioPreprocessor:\n    def __init__(self):\n        self.sample_rate = 16000\n\n    def preprocess_audio(self, raw_audio: bytes) -> bytes:\n        """Preprocess audio to optimize for Whisper"""\n        # Convert bytes to numpy array\n        audio_array = np.frombuffer(raw_audio, dtype=np.int16)\n\n        # Apply noise reduction (simple spectral gating)\n        audio_filtered = self._apply_noise_reduction(audio_array)\n\n        # Normalize audio levels\n        audio_normalized = self._normalize_audio(audio_filtered)\n\n        # Convert back to bytes\n        return audio_normalized.tobytes()\n\n    def _apply_noise_reduction(self, audio_array: np.ndarray) -> np.ndarray:\n        """Apply basic noise reduction"""\n        # Simple noise reduction by spectral gating\n        # This is a simplified version - real implementation would be more sophisticated\n        return audio_array  # Placeholder for actual noise reduction\n\n    def _normalize_audio(self, audio_array: np.ndarray) -> np.ndarray:\n        """Normalize audio to optimal levels for Whisper"""\n        # Find max amplitude\n        max_amplitude = np.max(np.abs(audio_array))\n\n        # Normalize to 70% of maximum to prevent clipping\n        if max_amplitude > 0:\n            normalization_factor = 0.7 * (2**15) / max_amplitude\n            audio_array = audio_array * normalization_factor\n\n        return audio_array.astype(np.int16)\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Integration and Testing:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import speech_recognition as sr\nimport time\n\ndef test_whisper_integration():\n    """Test the complete Whisper integration"""\n    # Initialize components\n    whisper_integrator = WhisperIntegrator(api_key=os.getenv("OPENAI_API_KEY"))\n    audio_preprocessor = AudioPreprocessor()\n    recognizer = sr.Recognizer()\n    microphone = sr.Microphone()\n\n    with microphone as source:\n        recognizer.adjust_for_ambient_noise(source)\n\n    print("Testing Whisper integration...")\n    print("Say \'Move forward\' to test the system")\n\n    try:\n        with microphone as source:\n            audio = recognizer.listen(source, timeout=5)\n\n        # Get raw audio data\n        raw_audio = audio.get_raw_data()\n\n        # Preprocess audio\n        processed_audio = audio_preprocessor.preprocess_audio(raw_audio)\n\n        # Transcribe with Whisper\n        start_time = time.time()\n        result = whisper_integrator.transcribe_audio_buffer(processed_audio)\n        end_time = time.time()\n\n        print(f"Transcription: {result}")\n        print(f"Processing time: {end_time - start_time:.2f} seconds")\n\n        # Check performance metrics\n        metrics = whisper_integrator.get_performance_metrics()\n        print(f"Average processing time: {metrics[\'avg_time\']:.2f} seconds")\n\n    except sr.WaitTimeoutError:\n        print("No speech detected")\n    except Exception as e:\n        print(f"Error during testing: {e}")\n\n# Run the test\ntest_whisper_integration()\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"exercise-tasks-1",children:"Exercise Tasks"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Implementation:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Create the ",(0,r.jsx)(e.code,{children:"WhisperIntegrator"})," class with proper error handling"]}),"\n",(0,r.jsxs)(e.li,{children:["Implement the ",(0,r.jsx)(e.code,{children:"AudioPreprocessor"})," with noise reduction and normalization"]}),"\n",(0,r.jsx)(e.li,{children:"Test the integration with your microphone input"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Optimization:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Add caching for repeated transcriptions"}),"\n",(0,r.jsx)(e.li,{children:"Implement rate limiting to handle API quotas"}),"\n",(0,r.jsx)(e.li,{children:"Add retry logic for failed requests"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Performance Testing:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Measure transcription times for different audio lengths"}),"\n",(0,r.jsx)(e.li,{children:"Test with different audio quality inputs"}),"\n",(0,r.jsx)(e.li,{children:"Compare results with and without preprocessing"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"expected-output-1",children:"Expected Output"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Testing Whisper integration...\nSay 'Move forward' to test the system\nTranscription: Move forward\nProcessing time: 1.25 seconds\nAverage processing time: 1.25 seconds\n"})}),"\n",(0,r.jsx)(e.h3,{id:"advanced-challenge",children:"Advanced Challenge"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Implement a streaming audio processor that can send partial audio to Whisper for faster response"}),"\n",(0,r.jsx)(e.li,{children:"Add support for multiple Whisper models and compare quality vs. speed"}),"\n",(0,r.jsx)(e.li,{children:"Create a fallback system that uses a local ASR model when Whisper is unavailable"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"exercise-3-command-mapping-exercise",children:"Exercise 3: Command Mapping Exercise"}),"\n",(0,r.jsx)(e.h3,{id:"objective-2",children:"Objective"}),"\n",(0,r.jsx)(e.p,{children:"Implement sophisticated command mapping that converts natural language to robot actions with context awareness and parameter extraction."}),"\n",(0,r.jsx)(e.h3,{id:"prerequisites-2",children:"Prerequisites"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Completed Whisper integration from Exercise 2"}),"\n",(0,r.jsx)(e.li,{children:"Understanding of natural language processing concepts"}),"\n",(0,r.jsx)(e.li,{children:"Basic knowledge of regular expressions"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"instructions-2",children:"Instructions"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Advanced Command Mapper with Context Awareness:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import re\nfrom typing import Dict, List, Optional, Any\nfrom dataclasses import dataclass\n\n@dataclass\nclass RobotAction:\n    action_type: str\n    parameters: Dict[str, Any]\n    confidence: float = 1.0\n    context: Dict[str, Any] = None\n\nclass AdvancedCommandMapper:\n    def __init__(self):\n        self.context = {}\n        self.command_patterns = [\n            # Navigation patterns\n            {\n                "pattern": r"move\\s+(forward|backward|ahead|back)\\s+(\\d+(?:\\.\\d+)?)\\s*(?:meters?|m|units?)",\n                "action_type": "navigation/move_distance",\n                "extractors": [\n                    lambda m: ("direction", "forward" if m.group(1) in ["forward", "ahead"] else "backward"),\n                    lambda m: ("distance", float(m.group(2)))\n                ]\n            },\n            {\n                "pattern": r"turn\\s+(left|right)\\s+(\\d+(?:\\.\\d+)?)\\s*(?:degrees?|deg)",\n                "action_type": "navigation/turn_angle",\n                "extractors": [\n                    lambda m: ("angle", float(m.group(2)) if m.group(1) == "right" else -float(m.group(2)))\n                ]\n            },\n            {\n                "pattern": r"go\\s+to\\s+(?:the\\s+)?(\\w+)",\n                "action_type": "navigation/go_to_location",\n                "extractors": [\n                    lambda m: ("location", m.group(1))\n                ]\n            },\n            # Manipulation patterns\n            {\n                "pattern": r"(?:pick\\s+up|grasp|take)\\s+(?:the\\s+)?(\\w+)",\n                "action_type": "manipulation/pick_object",\n                "extractors": [\n                    lambda m: ("object_type", m.group(1))\n                ]\n            },\n            {\n                "pattern": r"place\\s+(?:the\\s+)?(\\w+)\\s+(?:on|at|in)\\s+(?:the\\s+)?(\\w+)",\n                "action_type": "manipulation/place_object",\n                "extractors": [\n                    lambda m: ("object_type", m.group(1)),\n                    lambda m: ("destination", m.group(2))\n                ]\n            },\n            # Simple commands\n            {\n                "pattern": r"stop",\n                "action_type": "control/stop",\n                "extractors": []\n            },\n            {\n                "pattern": r"pause",\n                "action_type": "control/pause",\n                "extractors": []\n            },\n            {\n                "pattern": r"continue|resume",\n                "action_type": "control/resume",\n                "extractors": []\n            }\n        ]\n\n    def map_command(self, command_text: str) -> Optional[RobotAction]:\n        """Map command text to robot action with parameter extraction"""\n        command_lower = command_text.lower().strip()\n\n        for pattern_config in self.command_patterns:\n            match = re.search(pattern_config["pattern"], command_lower)\n            if match:\n                parameters = {}\n                for extractor in pattern_config["extractors"]:\n                    key, value = extractor(match)\n                    parameters[key] = value\n\n                return RobotAction(\n                    action_type=pattern_config["action_type"],\n                    parameters=parameters,\n                    confidence=0.9  # High confidence for pattern matches\n                )\n\n        # If no pattern matches, try fuzzy matching\n        return self._fuzzy_match_command(command_lower)\n\n    def _fuzzy_match_command(self, command_text: str) -> Optional[RobotAction]:\n        """Use fuzzy matching for commands that don\'t match patterns exactly"""\n        # Simple fuzzy matching - in practice, use more sophisticated NLP\n        known_commands = {\n            "move forward": ("navigation/move_forward", {"distance": 1.0}),\n            "move backward": ("navigation/move_backward", {"distance": 1.0}),\n            "turn left": ("navigation/turn", {"angle": -90}),\n            "turn right": ("navigation/turn", {"angle": 90}),\n            "stop": ("control/stop", {}),\n            "pick object": ("manipulation/pick_object", {}),\n            "place object": ("manipulation/place_object", {})\n        }\n\n        best_match = None\n        best_score = 0\n\n        for known_cmd, (action_type, params) in known_commands.items():\n            score = self._calculate_similarity(command_text, known_cmd)\n            if score > best_score and score > 0.7:  # 70% similarity threshold\n                best_match = (action_type, params)\n                best_score = score\n\n        if best_match:\n            return RobotAction(\n                action_type=best_match[0],\n                parameters=best_match[1],\n                confidence=best_score\n            )\n\n        return RobotAction(\n            action_type="unknown/command",\n            parameters={"text": command_text},\n            confidence=0.0\n        )\n\n    def _calculate_similarity(self, str1: str, str2: str) -> float:\n        """Calculate similarity between two strings"""\n        from difflib import SequenceMatcher\n        return SequenceMatcher(None, str1, str2).ratio()\n\n    def set_context(self, context: Dict[str, Any]):\n        """Set current robot context for contextual command mapping"""\n        self.context = context\n\n    def get_context_aware_mapping(self, command_text: str) -> Optional[RobotAction]:\n        """Map command considering current context"""\n        # Apply context-specific rules\n        action = self.map_command(command_text)\n\n        # Modify action based on context if needed\n        if self.context.get("location") == "kitchen" and "go to" in command_text.lower():\n            # In kitchen context, "go to" might mean different things\n            if "fridge" in command_text.lower():\n                action.parameters["location"] = "kitchen_fridge"\n            elif "counter" in command_text.lower():\n                action.parameters["location"] = "kitchen_counter"\n\n        return action\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Confidence-Based Command Processing:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'class ConfidenceBasedProcessor:\n    def __init__(self, threshold: float = 0.7):\n        self.threshold = threshold\n        self.command_mapper = AdvancedCommandMapper()\n\n    def process_command_with_confidence(self, command_text: str) -> Dict[str, Any]:\n        """Process command with confidence scoring and validation"""\n        action = self.command_mapper.map_command(command_text)\n\n        if action.confidence < self.threshold:\n            return {\n                "status": "uncertain",\n                "action": action,\n                "suggestions": self._generate_suggestions(command_text),\n                "confidence": action.confidence\n            }\n\n        # Validate parameters\n        validation_result = self._validate_action_parameters(action)\n        if not validation_result["valid"]:\n            return {\n                "status": "invalid_parameters",\n                "action": action,\n                "errors": validation_result["errors"],\n                "confidence": action.confidence\n            }\n\n        return {\n            "status": "confirmed",\n            "action": action,\n            "confidence": action.confidence\n        }\n\n    def _validate_action_parameters(self, action: RobotAction) -> Dict[str, Any]:\n        """Validate action parameters based on action type"""\n        errors = []\n\n        if action.action_type == "navigation/move_distance":\n            if "distance" not in action.parameters:\n                errors.append("Distance parameter is required for move commands")\n            elif not isinstance(action.parameters["distance"], (int, float)):\n                errors.append("Distance must be a number")\n            elif action.parameters["distance"] <= 0:\n                errors.append("Distance must be positive")\n\n        elif action.action_type == "navigation/turn_angle":\n            if "angle" not in action.parameters:\n                errors.append("Angle parameter is required for turn commands")\n            elif not isinstance(action.parameters["angle"], (int, float)):\n                errors.append("Angle must be a number")\n\n        elif action.action_type == "navigation/go_to_location":\n            if "location" not in action.parameters:\n                errors.append("Location parameter is required for navigation commands")\n\n        return {\n            "valid": len(errors) == 0,\n            "errors": errors\n        }\n\n    def _generate_suggestions(self, command_text: str) -> List[str]:\n        """Generate suggestions for uncertain commands"""\n        suggestions = []\n\n        # Common command patterns that might be similar\n        common_commands = [\n            "move forward",\n            "move backward",\n            "turn left",\n            "turn right",\n            "go to kitchen",\n            "pick up object",\n            "place object",\n            "stop"\n        ]\n\n        for cmd in common_commands:\n            similarity = self.command_mapper._calculate_similarity(command_text.lower(), cmd)\n            if similarity > 0.5:  # 50% similarity\n                suggestions.append(cmd)\n\n        return suggestions\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Integration and Testing:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def test_command_mapping():\n    """Test the command mapping system"""\n    mapper = AdvancedCommandMapper()\n    processor = ConfidenceBasedProcessor(threshold=0.6)\n\n    test_commands = [\n        "move forward 2 meters",\n        "turn left 90 degrees",\n        "pick up the red ball",\n        "place the cup on the table",\n        "go to the kitchen",\n        "stop immediately",\n        "move backwards 1.5 meters"\n    ]\n\n    print("Testing Command Mapping System")\n    print("=" * 40)\n\n    for command in test_commands:\n        print(f"\\nCommand: \'{command}\'")\n        result = processor.process_command_with_confidence(command)\n\n        if result["status"] == "confirmed":\n            print(f"  Action: {result[\'action\'].action_type}")\n            print(f"  Parameters: {result[\'action\'].parameters}")\n            print(f"  Confidence: {result[\'action\'].confidence:.2f}")\n        elif result["status"] == "uncertain":\n            print(f"  Status: Uncertain (confidence: {result[\'action\'].confidence:.2f})")\n            if result["suggestions"]:\n                print(f"  Suggestions: {\', \'.join(result[\'suggestions\'])}")\n        elif result["status"] == "invalid_parameters":\n            print(f"  Status: Invalid parameters")\n            print(f"  Errors: {result[\'errors\']}")\n\n# Run the test\ntest_command_mapping()\n'})}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Context-Aware Command Processing:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'def test_context_aware_mapping():\n    """Test command mapping with context awareness"""\n    mapper = AdvancedCommandMapper()\n\n    # Set initial context\n    context = {\n        "location": "kitchen",\n        "objects_nearby": ["cup", "plate", "apple"],\n        "robot_state": "idle"\n    }\n    mapper.set_context(context)\n\n    print("\\nTesting Context-Aware Command Mapping")\n    print("=" * 40)\n\n    contextual_commands = [\n        "pick up the apple",  # Should recognize apple is nearby\n        "go to the counter",  # Context-aware navigation\n        "place the apple on the plate"  # Multi-object interaction\n    ]\n\n    for command in contextual_commands:\n        print(f"\\nCommand: \'{command}\'")\n        print(f"Context: {mapper.context}")\n        action = mapper.get_context_aware_mapping(command)\n        print(f"  Action: {action.action_type}")\n        print(f"  Parameters: {action.parameters}")\n        print(f"  Confidence: {action.confidence:.2f}")\n'})}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"exercise-tasks-2",children:"Exercise Tasks"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Implementation:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Create the ",(0,r.jsx)(e.code,{children:"AdvancedCommandMapper"})," class with pattern matching"]}),"\n",(0,r.jsxs)(e.li,{children:["Implement the ",(0,r.jsx)(e.code,{children:"ConfidenceBasedProcessor"})," with validation"]}),"\n",(0,r.jsx)(e.li,{children:"Test with various command formats and edge cases"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Extension:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'Add support for temporal commands (e.g., "wait for 5 seconds")'}),"\n",(0,r.jsx)(e.li,{children:'Implement command chaining (e.g., "move forward then turn left")'}),"\n",(0,r.jsx)(e.li,{children:'Add natural language quantifiers ("a little", "a lot", "very")'}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Validation:"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Create a test suite with various command formats"}),"\n",(0,r.jsx)(e.li,{children:"Test edge cases and error conditions"}),"\n",(0,r.jsx)(e.li,{children:"Validate parameter ranges and types"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"expected-output-2",children:"Expected Output"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Testing Command Mapping System\n========================================\n\nCommand: 'move forward 2 meters'\n  Action: navigation/move_distance\n  Parameters: {'direction': 'forward', 'distance': 2.0}\n  Confidence: 0.90\n\nCommand: 'turn left 90 degrees'\n  Action: navigation/turn_angle\n  Parameters: {'angle': -90.0}\n  Confidence: 0.90\n\nCommand: 'pick up the red ball'\n  Action: manipulation/pick_object\n  Parameters: {'object_type': 'red ball'}\n  Confidence: 0.90\n"})}),"\n",(0,r.jsx)(e.h3,{id:"advanced-challenge-1",children:"Advanced Challenge"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Implement a learning system that improves command mapping based on user corrections"}),"\n",(0,r.jsx)(e.li,{children:"Add support for multi-language commands"}),"\n",(0,r.jsx)(e.li,{children:"Create a command history system that can learn user preferences"}),"\n",(0,r.jsx)(e.li,{children:"Implement safety validation for dangerous commands"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"exercise-4-performance-optimization-and-troubleshooting",children:"Exercise 4: Performance Optimization and Troubleshooting"}),"\n",(0,r.jsx)(e.h3,{id:"objective-3",children:"Objective"}),"\n",(0,r.jsx)(e.p,{children:"Optimize the voice-to-action pipeline for performance and implement comprehensive troubleshooting strategies."}),"\n",(0,r.jsx)(e.h3,{id:"instructions-3",children:"Instructions"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Implement performance monitoring and optimization techniques"}),"\n",(0,r.jsx)(e.li,{children:"Create comprehensive error handling and troubleshooting guides"}),"\n",(0,r.jsx)(e.li,{children:"Develop system health checks and diagnostic tools"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"This exercise focuses on making the system robust and performant in real-world conditions."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(l,{...n})}):l(n)}},8453(n,e,t){t.d(e,{R:()=>o,x:()=>s});var i=t(6540);const r={},a=i.createContext(r);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);