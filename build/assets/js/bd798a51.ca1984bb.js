"use strict";(globalThis.webpackChunkros2_book=globalThis.webpackChunkros2_book||[]).push([[186],{8453(n,e,i){i.d(e,{R:()=>s,x:()=>r});var o=i(6540);const t={},a=o.createContext(t);function s(n){const e=o.useContext(a);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),o.createElement(a.Provider,{value:e},n.children)}},8785(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module2/solutions/sensor_simulation_exercise_solutions","title":"Sensor Simulation Exercise Solutions","description":"Exercise 1 Solution: LiDAR Configuration","source":"@site/docs/module2/solutions/sensor_simulation_exercise_solutions.md","sourceDirName":"module2/solutions","slug":"/module2/solutions/sensor_simulation_exercise_solutions","permalink":"/docs/module2/solutions/sensor_simulation_exercise_solutions","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module2/solutions/sensor_simulation_exercise_solutions.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Unity High-Fidelity Rendering & Human-Robot Interaction","permalink":"/docs/module2/unity-rendering-hri"},"next":{"title":"Isaac Sim Introduction","permalink":"/docs/isaac-sim/introduction"}}');var t=i(4848),a=i(8453);const s={},r="Sensor Simulation Exercise Solutions",l={},c=[{value:"Exercise 1 Solution: LiDAR Configuration",id:"exercise-1-solution-lidar-configuration",level:2},{value:"Problem",id:"problem",level:3},{value:"Solution",id:"solution",level:3},{value:"Key Points:",id:"key-points",level:3},{value:"Exercise 2 Solution: Depth Camera Setup",id:"exercise-2-solution-depth-camera-setup",level:2},{value:"Problem",id:"problem-1",level:3},{value:"Solution",id:"solution-1",level:3},{value:"Key Points:",id:"key-points-1",level:3},{value:"Exercise 3 Solution: IMU with Noise Model",id:"exercise-3-solution-imu-with-noise-model",level:2},{value:"Problem",id:"problem-2",level:3},{value:"Solution",id:"solution-2",level:3},{value:"Key Points:",id:"key-points-2",level:3},{value:"Exercise 4 Solution: Unity Point Cloud Visualization",id:"exercise-4-solution-unity-point-cloud-visualization",level:2},{value:"Problem",id:"problem-3",level:3},{value:"Solution",id:"solution-3",level:3},{value:"Key Points:",id:"key-points-3",level:3},{value:"Exercise 5 Solution: Sensor Fusion Implementation",id:"exercise-5-solution-sensor-fusion-implementation",level:2},{value:"Problem",id:"problem-4",level:3},{value:"Solution",id:"solution-4",level:3},{value:"Key Points:",id:"key-points-4",level:3},{value:"Exercise 6 Solution: Unity Sensor Data Visualization",id:"exercise-6-solution-unity-sensor-data-visualization",level:2},{value:"Problem",id:"problem-5",level:3},{value:"Solution",id:"solution-5",level:3},{value:"Key Points:",id:"key-points-5",level:3},{value:"Summary of Exercise Solutions",id:"summary-of-exercise-solutions",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"sensor-simulation-exercise-solutions",children:"Sensor Simulation Exercise Solutions"})}),"\n",(0,t.jsx)(e.h2,{id:"exercise-1-solution-lidar-configuration",children:"Exercise 1 Solution: LiDAR Configuration"}),"\n",(0,t.jsx)(e.h3,{id:"problem",children:"Problem"}),"\n",(0,t.jsx)(e.p,{children:"Configure a LiDAR sensor with 360\xb0 horizontal field of view, 16 vertical channels, and 20m maximum range."}),"\n",(0,t.jsx)(e.h3,{id:"solution",children:"Solution"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar_360" type="ray">\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>360</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>  \x3c!-- -\u03c0 radians (-180\xb0) --\x3e\n        <max_angle>3.14159</max_angle>   \x3c!-- \u03c0 radians (180\xb0) --\x3e\n      </horizontal>\n      <vertical>\n        <samples>16</samples>\n        <resolution>1</resolution>\n        <min_angle>-0.261799</min_angle>  \x3c!-- -15\xb0 in radians --\x3e\n        <max_angle>0.261799</max_angle>   \x3c!-- 15\xb0 in radians --\x3e\n      </vertical>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>20.0</max>  \x3c!-- 20m maximum range --\x3e\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="lidar_controller" filename="libRayPlugin.so">\n    <always_on>true</always_on>\n    <update_rate>10</update_rate>\n    <topic_name>/laser_scan</topic_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,t.jsx)(e.h3,{id:"key-points",children:"Key Points:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Horizontal samples: 360 for full 360\xb0 coverage"}),"\n",(0,t.jsx)(e.li,{children:"Min/max angles: \xb1\u03c0 radians for full circle"}),"\n",(0,t.jsx)(e.li,{children:"Vertical channels: 16 for elevation coverage"}),"\n",(0,t.jsx)(e.li,{children:"Range: 0.1m minimum to 20m maximum"}),"\n",(0,t.jsx)(e.li,{children:"Update rate: 10Hz for real-time performance"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"exercise-2-solution-depth-camera-setup",children:"Exercise 2 Solution: Depth Camera Setup"}),"\n",(0,t.jsx)(e.h3,{id:"problem-1",children:"Problem"}),"\n",(0,t.jsx)(e.p,{children:"Create a depth camera with 640\xd7480 resolution, 60\xb0 horizontal FOV, and realistic noise parameters."}),"\n",(0,t.jsx)(e.h3,{id:"solution-1",children:"Solution"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60\xb0 in radians --\x3e\n    <image>\n      <width>640</width>\n      <height>480</height>\n      <format>R8G8B8</format>\n    </image>\n    <clip>\n      <near>0.1</near>  \x3c!-- 0.1m minimum range --\x3e\n      <far>10.0</far>   \x3c!-- 10m maximum range --\x3e\n    </clip>\n    <noise>\n      <type>gaussian</type>\n      <mean>0.0</mean>\n      <stddev>0.01</stddev>  \x3c!-- Realistic depth noise --\x3e\n    </noise>\n  </camera>\n  <plugin name="camera_controller" filename="libDepthCameraPlugin.so">\n    <always_on>true</always_on>\n    <update_rate>30</update_rate>\n    <topic_name>/camera/depth/image_raw</topic_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,t.jsx)(e.h3,{id:"key-points-1",children:"Key Points:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Image dimensions: 640\xd7480 pixels"}),"\n",(0,t.jsx)(e.li,{children:"Horizontal FOV: 60\xb0 (1.047 radians)"}),"\n",(0,t.jsx)(e.li,{children:"Near/far clipping: 0.1m to 10m range"}),"\n",(0,t.jsx)(e.li,{children:"Noise: Gaussian with 1cm standard deviation"}),"\n",(0,t.jsx)(e.li,{children:"Update rate: 30Hz for video-like performance"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"exercise-3-solution-imu-with-noise-model",children:"Exercise 3 Solution: IMU with Noise Model"}),"\n",(0,t.jsx)(e.h3,{id:"problem-2",children:"Problem"}),"\n",(0,t.jsx)(e.p,{children:"Configure an IMU sensor with realistic noise parameters for angular velocity and linear acceleration."}),"\n",(0,t.jsx)(e.h3,{id:"solution-2",children:"Solution"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\n  <imu>\n    <angular_velocity>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>  \x3c!-- ~0.1\xb0/s standard deviation --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.0017</stddev>\n        </noise>\n      </z>\n    </angular_velocity>\n    <linear_acceleration>\n      <x>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>  \x3c!-- ~0.017 m/s\xb2 standard deviation --\x3e\n        </noise>\n      </x>\n      <y>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>\n        </noise>\n      </y>\n      <z>\n        <noise type="gaussian">\n          <mean>0.0</mean>\n          <stddev>0.017</stddev>\n        </noise>\n      </z>\n    </linear_acceleration>\n  </imu>\n  <plugin name="imu_controller" filename="libImuPlugin.so">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <topic_name>/imu/data</topic_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,t.jsx)(e.h3,{id:"key-points-2",children:"Key Points:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Angular velocity noise: ~0.1\xb0/s standard deviation (realistic for tactical-grade IMUs)"}),"\n",(0,t.jsx)(e.li,{children:"Linear acceleration noise: ~0.017 m/s\xb2 standard deviation"}),"\n",(0,t.jsx)(e.li,{children:"Update rate: 100Hz for high-frequency measurements"}),"\n",(0,t.jsx)(e.li,{children:"Gaussian noise model for realistic sensor behavior"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"exercise-4-solution-unity-point-cloud-visualization",children:"Exercise 4 Solution: Unity Point Cloud Visualization"}),"\n",(0,t.jsx)(e.h3,{id:"problem-3",children:"Problem"}),"\n",(0,t.jsx)(e.p,{children:"Create a Unity script that visualizes point cloud data as individual points in the scene."}),"\n",(0,t.jsx)(e.h3,{id:"solution-3",children:"Solution"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections.Generic;\n\npublic class PointCloudVisualizer : MonoBehaviour\n{\n    [Header("Visualization Settings")]\n    public GameObject pointPrefab;\n    public float pointSize = 0.01f;\n    public Color pointColor = Color.red;\n    public int maxPoints = 10000;\n\n    [Header("Performance")]\n    public bool useObjectPooling = true;\n    public int poolSize = 1000;\n\n    private List<GameObject> pointPool;\n    private List<GameObject> activePoints;\n    private int poolIndex = 0;\n\n    void Start()\n    {\n        InitializePointPool();\n    }\n\n    void InitializePointPool()\n    {\n        if (useObjectPooling)\n        {\n            pointPool = new List<GameObject>();\n            activePoints = new List<GameObject>();\n\n            // Create pooled objects\n            for (int i = 0; i < poolSize; i++)\n            {\n                GameObject point = CreatePoint();\n                point.SetActive(false);\n                pointPool.Add(point);\n            }\n        }\n    }\n\n    GameObject CreatePoint()\n    {\n        GameObject point;\n        if (pointPrefab != null)\n        {\n            point = Instantiate(pointPrefab, Vector3.zero, Quaternion.identity);\n        }\n        else\n        {\n            // Create default sphere if no prefab provided\n            point = GameObject.CreatePrimitive(PrimitiveType.Sphere);\n            point.GetComponent<Renderer>().material = new Material(Shader.Find("Universal Render Pipeline/Lit"));\n            point.GetComponent<Renderer>().material.color = pointColor;\n        }\n\n        point.transform.SetParent(transform);\n        point.transform.localScale = Vector3.one * pointSize;\n        return point;\n    }\n\n    public void UpdatePointCloud(List<Vector3> points)\n    {\n        if (useObjectPooling)\n        {\n            UpdateWithPooling(points);\n        }\n        else\n        {\n            UpdateWithoutPooling(points);\n        }\n    }\n\n    void UpdateWithPooling(List<Vector3> points)\n    {\n        // Deactivate all previously active points\n        foreach (GameObject activePoint in activePoints)\n        {\n            activePoint.SetActive(false);\n        }\n        activePoints.Clear();\n\n        // Activate and position pooled points\n        int pointsToShow = Mathf.Min(points.Count, poolSize);\n        for (int i = 0; i < pointsToShow; i++)\n        {\n            GameObject point = pointPool[poolIndex];\n            point.SetActive(true);\n            point.transform.position = points[i];\n            point.GetComponent<Renderer>().material.color = GetColorForHeight(points[i].y);\n\n            activePoints.Add(point);\n            poolIndex = (poolIndex + 1) % poolSize;\n        }\n    }\n\n    void UpdateWithoutPooling(List<Vector3> points)\n    {\n        // Destroy all previous points\n        foreach (Transform child in transform)\n        {\n            Destroy(child.gameObject);\n        }\n\n        // Create new points\n        int pointsToShow = Mathf.Min(points.Count, maxPoints);\n        for (int i = 0; i < pointsToShow; i++)\n        {\n            GameObject point = CreatePoint();\n            point.transform.position = points[i];\n            point.GetComponent<Renderer>().material.color = GetColorForHeight(points[i].y);\n        }\n    }\n\n    Color GetColorForHeight(float height)\n    {\n        // Map height to color gradient (blue to red)\n        float normalizedHeight = Mathf.InverseLerp(-5f, 5f, height);\n        return Color.Lerp(Color.blue, Color.red, normalizedHeight);\n    }\n\n    // Helper method to generate sample point cloud\n    public void GenerateSamplePointCloud()\n    {\n        List<Vector3> points = new List<Vector3>();\n\n        // Generate a spherical point cloud\n        for (int i = 0; i < 1000; i++)\n        {\n            float theta = Random.Range(0f, 2f * Mathf.PI);\n            float phi = Random.Range(0f, Mathf.PI);\n            float radius = Random.Range(1f, 3f);\n\n            float x = radius * Mathf.Sin(phi) * Mathf.Cos(theta);\n            float y = radius * Mathf.Sin(phi) * Mathf.Sin(theta);\n            float z = radius * Mathf.Cos(phi);\n\n            points.Add(new Vector3(x, y, z));\n        }\n\n        UpdatePointCloud(points);\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"key-points-3",children:"Key Points:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Object pooling for performance optimization"}),"\n",(0,t.jsx)(e.li,{children:"Height-based coloring for visual clarity"}),"\n",(0,t.jsx)(e.li,{children:"Configurable point size and color"}),"\n",(0,t.jsx)(e.li,{children:"Efficient update mechanisms for large point clouds"}),"\n",(0,t.jsx)(e.li,{children:"Sample generation for testing purposes"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"exercise-5-solution-sensor-fusion-implementation",children:"Exercise 5 Solution: Sensor Fusion Implementation"}),"\n",(0,t.jsx)(e.h3,{id:"problem-4",children:"Problem"}),"\n",(0,t.jsx)(e.p,{children:"Implement a sensor fusion node that combines LiDAR and IMU data to improve robot localization."}),"\n",(0,t.jsx)(e.h3,{id:"solution-4",children:"Solution"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSensor Fusion Node for Robot Localization\n\nThis node combines LiDAR and IMU data to provide improved pose estimation.\n"""\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import Pose, Twist\nfrom tf2_ros import TransformBroadcaster\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\n\nclass SensorFusionNode(Node):\n    def __init__(self):\n        super().__init__(\'sensor_fusion_node\')\n\n        # Subscriptions\n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.lidar_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # Publisher\n        self.odom_pub = self.create_publisher(Odometry, \'/odom_fused\', 10)\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # State variables\n        self.robot_pose = np.array([0.0, 0.0, 0.0])  # x, y, theta\n        self.robot_twist = np.array([0.0, 0.0, 0.0])  # vx, vy, omega\n        self.last_imu_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # quaternion\n\n        # Covariance matrices (simplified)\n        self.pose_covariance = np.eye(6) * 0.1  # Initial uncertainty\n        self.twist_covariance = np.eye(6) * 0.05\n\n        # Timing\n        self.last_update_time = self.get_clock().now()\n\n        self.get_logger().info(\'Sensor fusion node initialized\')\n\n    def lidar_callback(self, msg):\n        """Process LiDAR data for position estimation."""\n        # Extract features from LiDAR scan (simplified approach)\n        # In practice, this would involve feature extraction, ICP, etc.\n\n        # For this example, we\'ll use a simple approach to estimate movement\n        # based on changes in the environment\n        self.process_lidar_data(msg)\n\n    def imu_callback(self, msg):\n        """Process IMU data for orientation and acceleration."""\n        # Extract orientation from IMU\n        quat = np.array([\n            msg.orientation.x,\n            msg.orientation.y,\n            msg.orientation.z,\n            msg.orientation.w\n        ])\n\n        # Convert to Euler angles for simple processing\n        rotation = R.from_quat(quat)\n        euler_angles = rotation.as_euler(\'xyz\')\n\n        # Update orientation in state\n        self.last_imu_orientation = quat\n        self.robot_pose[2] = euler_angles[2]  # Update theta (yaw)\n\n        # Extract angular velocity and linear acceleration\n        angular_vel = np.array([\n            msg.angular_velocity.x,\n            msg.angular_velocity.y,\n            msg.angular_velocity.z\n        ])\n\n        linear_acc = np.array([\n            msg.linear_acceleration.x,\n            msg.linear_acceleration.y,\n            msg.linear_acceleration.z\n        ])\n\n        # Integrate to get velocity and position (simplified)\n        current_time = self.get_clock().now()\n        dt = (current_time - self.last_update_time).nanoseconds / 1e9\n        self.last_update_time = current_time\n\n        if dt > 0:\n            # Update twist based on IMU acceleration\n            self.robot_twist[2] = angular_vel[2]  # Angular velocity around Z\n\n            # Integrate linear acceleration to get velocity (in robot frame)\n            local_acc = np.array([linear_acc[0], linear_acc[1], 0])\n\n            # Transform to global frame using current orientation\n            cos_th = np.cos(self.robot_pose[2])\n            sin_th = np.sin(self.robot_pose[2])\n            R_global = np.array([[cos_th, -sin_th, 0],\n                                [sin_th, cos_th, 0],\n                                [0, 0, 1]])\n\n            global_acc = R_global @ local_acc\n            self.robot_twist[0:2] += global_acc[0:2] * dt\n\n            # Update position based on velocity\n            self.robot_pose[0:2] += self.robot_twist[0:2] * dt\n\n    def process_lidar_data(self, scan_msg):\n        """Process LiDAR data for position refinement."""\n        # This is a simplified approach - in practice, you would:\n        # 1. Extract features (corners, edges, planes) from the scan\n        # 2. Match features with previous scan (ICP or similar)\n        # 3. Estimate transformation between scans\n        # 4. Fuse with IMU data using a Kalman filter or particle filter\n\n        # For this example, we\'ll just use the data to adjust confidence\n        # in our pose estimate based on environmental consistency\n\n        # Calculate average distance to assess environment openness\n        valid_ranges = [r for r in scan_msg.ranges if scan_msg.range_min <= r <= scan_msg.range_max]\n        if valid_ranges:\n            avg_distance = sum(valid_ranges) / len(valid_ranges)\n\n            # Adjust covariance based on environmental features\n            # More features generally mean better position confidence\n            if avg_distance < 2.0:  # Close environment - more features available\n                self.pose_covariance[:2, :2] *= 0.8  # Reduce position uncertainty\n            else:  # Open environment - fewer features\n                self.pose_covariance[:2, :2] *= 1.2  # Increase position uncertainty\n\n    def publish_odometry(self):\n        """Publish fused odometry data."""\n        current_time = self.get_clock().now()\n        odom_msg = Odometry()\n        odom_msg.header.stamp = current_time.to_msg()\n        odom_msg.header.frame_id = \'odom\'\n        odom_msg.child_frame_id = \'base_link\'\n\n        # Set pose\n        odom_msg.pose.pose.position.x = float(self.robot_pose[0])\n        odom_msg.pose.pose.position.y = float(self.robot_pose[1])\n        odom_msg.pose.pose.position.z = 0.0\n\n        # Convert yaw to quaternion\n        yaw = self.robot_pose[2]\n        odom_msg.pose.pose.orientation.x = 0.0\n        odom_msg.pose.pose.orientation.y = 0.0\n        odom_msg.pose.pose.orientation.z = np.sin(yaw * 0.5)\n        odom_msg.pose.pose.orientation.w = np.cos(yaw * 0.5)\n\n        # Set pose covariance\n        odom_msg.pose.covariance = self.pose_covariance.flatten().tolist()\n\n        # Set twist\n        odom_msg.twist.twist.linear.x = float(self.robot_twist[0])\n        odom_msg.twist.twist.linear.y = float(self.robot_twist[1])\n        odom_msg.twist.twist.angular.z = float(self.robot_twist[2])\n\n        # Set twist covariance\n        odom_msg.twist.covariance = self.twist_covariance.flatten().tolist()\n\n        # Publish message\n        self.odom_pub.publish(odom_msg)\n\n        # Broadcast transform\n        self.broadcast_transform(current_time, odom_msg.pose.pose)\n\n    def broadcast_transform(self, stamp, pose):\n        """Broadcast the transform from odom to base_link."""\n        from geometry_msgs.msg import TransformStamped\n\n        t = TransformStamped()\n        t.header.stamp = stamp.to_msg()\n        t.header.frame_id = \'odom\'\n        t.child_frame_id = \'base_link\'\n\n        t.transform.translation.x = pose.position.x\n        t.transform.translation.y = pose.position.y\n        t.transform.translation.z = pose.position.z\n\n        t.transform.rotation = pose.orientation\n\n        self.tf_broadcaster.sendTransform(t)\n\n    def timer_callback(self):\n        """Periodic callback to publish fused data."""\n        self.publish_odometry()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    try:\n        fusion_node = SensorFusionNode()\n\n        # Create timer for periodic publishing\n        timer = fusion_node.create_timer(0.05, fusion_node.timer_callback)  # 20 Hz\n\n        rclpy.spin(fusion_node)\n\n    except KeyboardInterrupt:\n        pass\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(e.h3,{id:"key-points-4",children:"Key Points:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Combines LiDAR and IMU data for improved localization"}),"\n",(0,t.jsx)(e.li,{children:"Implements basic sensor fusion algorithm"}),"\n",(0,t.jsx)(e.li,{children:"Publishes fused odometry with covariance"}),"\n",(0,t.jsx)(e.li,{children:"Uses proper ROS 2 message types and conventions"}),"\n",(0,t.jsx)(e.li,{children:"Includes TF broadcasting for coordinate transforms"}),"\n",(0,t.jsx)(e.li,{children:"Handles timing and data association properly"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"exercise-6-solution-unity-sensor-data-visualization",children:"Exercise 6 Solution: Unity Sensor Data Visualization"}),"\n",(0,t.jsx)(e.h3,{id:"problem-5",children:"Problem"}),"\n",(0,t.jsx)(e.p,{children:"Create a Unity script that visualizes different sensor modalities (LiDAR, camera, IMU) in a unified interface."}),"\n",(0,t.jsx)(e.h3,{id:"solution-5",children:"Solution"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\nusing TMPro;\nusing System.Collections.Generic;\n\npublic class MultiSensorVisualizer : MonoBehaviour\n{\n    [Header("Sensor Data Sources")]\n    public TextMeshProUGUI statusText;\n    public TextMeshProUGUI poseText;\n    public TextMeshProUGUI imuText;\n    public TextMeshProUGUI lidarText;\n\n    [Header("Visualization Elements")]\n    public PointCloudVisualizer pointCloudVisualizer;\n    public GameObject cameraFeedPanel;\n    public RawImage cameraTexture;\n    public GameObject imuIndicator;\n    public LineRenderer trajectoryLine;\n\n    [Header("Sensor Simulation")]\n    public bool simulateSensors = true;\n    public float simulationUpdateRate = 10f;\n\n    // Internal state\n    private float lastUpdateTime = 0f;\n    private List<Vector3> trajectoryPoints;\n    private Vector3 currentPose;\n    private Vector3 currentImuData;\n    private List<Vector3> simulatedPointCloud;\n\n    void Start()\n    {\n        trajectoryPoints = new List<Vector3>();\n        simulatedPointCloud = new List<Vector3>();\n\n        if (trajectoryLine != null)\n        {\n            trajectoryLine.positionCount = 0;\n        }\n\n        if (simulateSensors)\n        {\n            GenerateInitialPointCloud();\n        }\n    }\n\n    void Update()\n    {\n        if (simulateSensors && Time.time - lastUpdateTime > 1f / simulationUpdateRate)\n        {\n            UpdateSimulatedSensors();\n            lastUpdateTime = Time.time;\n        }\n\n        UpdateVisualizations();\n    }\n\n    void GenerateInitialPointCloud()\n    {\n        // Generate a simple environment point cloud\n        for (float x = -5f; x <= 5f; x += 0.5f)\n        {\n            for (float y = -5f; y <= 5f; y += 0.5f)\n            {\n                // Add ground points\n                simulatedPointCloud.Add(new Vector3(x, -0.5f, y));\n\n                // Add wall points occasionally\n                if (Mathf.Abs(x) > 4.5f || Mathf.Abs(y) > 4.5f)\n                {\n                    for (float z = 0f; z <= 3f; z += 0.5f)\n                    {\n                        simulatedPointCloud.Add(new Vector3(x, z, y));\n                    }\n                }\n            }\n        }\n    }\n\n    void UpdateSimulatedSensors()\n    {\n        // Update pose with simulated movement\n        currentPose += new Vector3(\n            Mathf.Sin(Time.time) * 0.01f,\n            0,\n            Mathf.Cos(Time.time) * 0.01f\n        );\n\n        // Add current position to trajectory\n        trajectoryPoints.Add(currentPose);\n        if (trajectoryPoints.Count > 1000) // Limit trajectory length\n        {\n            trajectoryPoints.RemoveAt(0);\n        }\n\n        // Update IMU simulation\n        currentImuData = new Vector3(\n            Mathf.Sin(Time.time * 2) * 0.1f,  // Linear acceleration X\n            Mathf.Cos(Time.time * 1.5f) * 0.1f,  // Linear acceleration Y\n            Mathf.Sin(Time.time * 0.8f) * 9.81f + 0.1f  // Linear acceleration Z (gravity + noise)\n        );\n\n        // Simulate IMU orientation change\n        if (imuIndicator != null)\n        {\n            imuIndicator.transform.rotation = Quaternion.Euler(\n                Mathf.Sin(Time.time * 0.5f) * 5f,  // Roll\n                Mathf.Cos(Time.time * 0.7f) * 5f,  // Pitch\n                Mathf.Sin(Time.time * 0.3f) * 10f  // Yaw\n            );\n        }\n\n        // Update point cloud with slight variations\n        if (pointCloudVisualizer != null)\n        {\n            // Add some random points to simulate sensor noise\n            List<Vector3> noisyPointCloud = new List<Vector3>(simulatedPointCloud);\n\n            for (int i = 0; i < 50; i++)\n            {\n                Vector3 randomNoise = new Vector3(\n                    Random.Range(-0.1f, 0.1f),\n                    Random.Range(-0.1f, 0.1f),\n                    Random.Range(-0.1f, 0.1f)\n                );\n\n                noisyPointCloud.Add(currentPose + randomNoise + new Vector3(\n                    Random.Range(-2f, 2f),\n                    Random.Range(-0.5f, 0.5f),\n                    Random.Range(-2f, 2f)\n                ));\n            }\n\n            pointCloudVisualizer.UpdatePointCloud(noisyPointCloud);\n        }\n    }\n\n    void UpdateVisualizations()\n    {\n        // Update status text\n        if (statusText != null)\n        {\n            statusText.text = $"Status: Active\\n" +\n                             $"Update Rate: {simulationUpdateRate:F1} Hz\\n" +\n                             $"Point Count: {(pointCloudVisualizer != null ? pointCloudVisualizer.GetPointCount() : 0)}";\n        }\n\n        // Update pose text\n        if (poseText != null)\n        {\n            poseText.text = $"Pose:\\n" +\n                           $"X: {currentPose.x:F2} m\\n" +\n                           $"Y: {currentPose.y:F2} m\\n" +\n                           $"Z: {currentPose.z:F2} m";\n        }\n\n        // Update IMU text\n        if (imuText != null)\n        {\n            imuText.text = $"IMU Data:\\n" +\n                          $"Acc X: {currentImuData.x:F3} m/s\xb2\\n" +\n                          $"Acc Y: {currentImuData.y:F3} m/s\xb2\\n" +\n                          $"Acc Z: {currentImuData.z:F3} m/s\xb2";\n        }\n\n        // Update LiDAR text\n        if (lidarText != null)\n        {\n            lidarText.text = $"LiDAR:\\n" +\n                            $"Points: {simulatedPointCloud.Count}\\n" +\n                            $"Range: 0.1 - 20.0 m\\n" +\n                            $"FOV: 360\xb0 H, 30\xb0 V";\n        }\n\n        // Update trajectory visualization\n        if (trajectoryLine != null && trajectoryPoints.Count > 1)\n        {\n            trajectoryLine.positionCount = trajectoryPoints.Count;\n            for (int i = 0; i < trajectoryPoints.Count; i++)\n            {\n                trajectoryLine.SetPosition(i, new Vector3(\n                    trajectoryPoints[i].x,\n                    0.1f,  // Slightly above ground for visibility\n                    trajectoryPoints[i].z\n                ));\n            }\n        }\n    }\n\n    public void ResetSimulation()\n    {\n        trajectoryPoints.Clear();\n        currentPose = Vector3.zero;\n        currentImuData = Vector3.zero;\n\n        if (trajectoryLine != null)\n        {\n            trajectoryLine.positionCount = 0;\n        }\n\n        if (pointCloudVisualizer != null)\n        {\n            pointCloudVisualizer.ClearPointCloud();\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(e.h3,{id:"key-points-5",children:"Key Points:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Integrates multiple sensor modalities in a unified interface"}),"\n",(0,t.jsx)(e.li,{children:"Visualizes point clouds, poses, and IMU data simultaneously"}),"\n",(0,t.jsx)(e.li,{children:"Includes trajectory tracking and visualization"}),"\n",(0,t.jsx)(e.li,{children:"Simulates realistic sensor behaviors with noise"}),"\n",(0,t.jsx)(e.li,{children:"Provides comprehensive status monitoring"}),"\n",(0,t.jsx)(e.li,{children:"Offers user controls for simulation reset"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary-of-exercise-solutions",children:"Summary of Exercise Solutions"}),"\n",(0,t.jsx)(e.p,{children:"These solutions demonstrate:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Proper sensor configuration"})," with realistic parameters"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Best practices"})," for sensor implementation in both Gazebo and Unity"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Integration techniques"})," for combining multiple sensor modalities"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Performance considerations"})," for real-time sensor processing"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Visualization methods"})," for sensor data interpretation"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"All solutions follow the educational contract standards with clear explanations, proper documentation, and realistic examples that match actual robotics applications."})]})}function u(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);